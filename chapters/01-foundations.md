<div align="center">

# ğŸ§  Foundations

![Chapter](https://img.shields.io/badge/Chapter-01-blue?style=for-the-badge)
![Topic](https://img.shields.io/badge/Topic-ML%20Basics%20%7C%20Math%20%7C%20Data-green?style=for-the-badge)
![Lines](https://img.shields.io/badge/Lines-4,949-orange?style=for-the-badge)

*Machine Learning Fundamentals, Mathematics & Neural Network Basics*

---

</div>

# PART I: FOUNDATIONS

---

# Chapter 1: Introduction to Machine Learning

> *"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E."*
> 
> â€” Tom Mitchell, 1997

---

## 1.1 What is Machine Learning?

Machine Learning (ML) is a subset of Artificial Intelligence that enables computers to learn patterns from data without being explicitly programmed for every possible scenario.

**The Key Insight:** Instead of writing rules for every situation, we show the computer examples and let it figure out the rules itself.

### Traditional Programming vs Machine Learning

**Traditional Programming:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚     â”‚                 â”‚     â”‚                 â”‚
â”‚      DATA       â”‚â”€â”€â”€â”€â–¶â”‚     RULES       â”‚â”€â”€â”€â”€â–¶â”‚     OUTPUT      â”‚
â”‚                 â”‚     â”‚  (hand-coded)   â”‚     â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Machine Learning:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚     â”‚                 â”‚     â”‚                 â”‚
â”‚      DATA       â”‚â”€â”€â”€â”€â–¶â”‚     MODEL       â”‚â”€â”€â”€â”€â–¶â”‚     RULES       â”‚
â”‚    + ANSWERS    â”‚     â”‚   (learning)    â”‚     â”‚   (discovered)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

A CONCRETE EXAMPLE: SPAM DETECTION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Traditional Approach:
You manually write rules like:
- IF email contains "Nigerian Prince" â†’ SPAM
- IF email contains "free money" â†’ SPAM
- IF sender not in contacts â†’ MAYBE SPAM
- ... hundreds more rules ...

Problems:
1. You can't anticipate every spam pattern
2. Spammers constantly change tactics
3. Rules might incorrectly flag legitimate emails
4. Maintaining rules is exhausting

Machine Learning Approach:
You collect 100,000 emails labeled as SPAM or NOT SPAM.
You feed them to a learning algorithm.
The algorithm discovers patterns:
- Certain word combinations
- Sender patterns
- Time patterns
- Link patterns
- ...patterns you never thought of...

Benefits:
1. Discovers patterns humans might miss
2. Adapts when retrained with new data
3. Can be more accurate than hand-coded rules
4. Scales to complex problems
"""

# Let's see both approaches in code:

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# EXAMPLE 1.1: Traditional Programming vs Machine Learning
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# TRADITIONAL APPROACH: Hand-coded rules
def is_spam_traditional(email_text, sender):
    """
    Traditional rule-based spam detection.
    
    Problems with this approach:
    1. We have to think of every rule
    2. Rules are rigid (what about "Fr33 M0n3y"?)
    3. Rules can conflict
    4. Hard to maintain as spam evolves
    """
    email_lower = email_text.lower()
    
    # Rule 1: Check for common spam phrases
    spam_phrases = [
        'nigerian prince',
        'free money',
        'click here now',
        'act immediately',
        'limited time offer',
        'you have won',
        'congratulations',
        'million dollars',
        'wire transfer',
        'urgent response needed'
    ]
    
    for phrase in spam_phrases:
        if phrase in email_lower:
            return True
    
    # Rule 2: Check for suspicious sender patterns
    suspicious_domains = ['spam.com', 'free-money.net', 'winner.org']
    for domain in suspicious_domains:
        if domain in sender.lower():
            return True
    
    # Rule 3: Check for excessive capitalization
    caps_ratio = sum(1 for c in email_text if c.isupper()) / max(len(email_text), 1)
    if caps_ratio > 0.5:
        return True
    
    # Rule 4: Check for excessive exclamation marks
    if email_text.count('!') > 5:
        return True
    
    return False


# MACHINE LEARNING APPROACH: Learn from data
def create_ml_spam_detector():
    """
    Machine Learning spam detection.
    
    The algorithm learns patterns from labeled examples.
    It can discover patterns we never thought of!
    """
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.naive_bayes import MultinomialNB
    from sklearn.pipeline import Pipeline
    
    # Sample training data (in reality, you'd have thousands)
    training_emails = [
        "Hey, want to grab lunch tomorrow?",
        "Meeting rescheduled to 3pm",
        "FREE MONEY!!! Click here to claim your prize!!!",
        "Congratulations! You've won $1,000,000",
        "Project deadline extended to Friday",
        "URGENT: Transfer money immediately",
        "Can you review the attached document?",
        "Limited time offer! Act now!",
        "Your Amazon order has shipped",
        "Nigerian prince needs your help",
    ]
    
    labels = [0, 0, 1, 1, 0, 1, 0, 1, 0, 1]  # 0=not spam, 1=spam
    
    # Create a pipeline that:
    # 1. Converts text to numerical features (TF-IDF)
    # 2. Trains a Naive Bayes classifier
    model = Pipeline([
        ('tfidf', TfidfVectorizer()),
        ('classifier', MultinomialNB())
    ])
    
    # Train the model (this is where the "learning" happens!)
    model.fit(training_emails, labels)
    
    return model


# Let's test both approaches
def compare_approaches():
    """Compare traditional vs ML approaches"""
    
    test_emails = [
        ("Special offer just for you! Limited time!", "promo@deals.com"),
        ("Can we reschedule our meeting?", "colleague@company.com"),
        ("YOU HAVE WON A LOTTERY!!!", "winner@lottery.com"),
        ("Quarterly report attached", "boss@company.com"),
    ]
    
    print("Traditional Approach Results:")
    print("-" * 50)
    for email, sender in test_emails:
        result = is_spam_traditional(email, sender)
        print(f"Email: '{email[:40]}...'")
        print(f"Spam: {result}\n")
    
    print("\nMachine Learning Approach Results:")
    print("-" * 50)
    ml_model = create_ml_spam_detector()
    for email, sender in test_emails:
        # Get probability of being spam
        proba = ml_model.predict_proba([email])[0]
        prediction = ml_model.predict([email])[0]
        print(f"Email: '{email[:40]}...'")
        print(f"Spam: {bool(prediction)} (confidence: {proba[prediction]:.1%})\n")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1.2 A BRIEF HISTORY OF MACHINE LEARNING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
A BRIEF HISTORY OF MACHINE LEARNING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Timeline of Key Developments:

1943 â”‚ Warren McCulloch & Walter Pitts
     â”‚ First mathematical model of a neural network
     â”‚ Showed neurons could implement logical functions
     â”‚
1950 â”‚ Alan Turing
     â”‚ "Computing Machinery and Intelligence"
     â”‚ Proposed the Turing Test
     â”‚ Asked "Can machines think?"
     â”‚
1957 â”‚ Frank Rosenblatt
     â”‚ The Perceptron
     â”‚ First trainable neural network
     â”‚ Could learn to classify simple patterns
     â”‚
1967 â”‚ The Nearest Neighbor Algorithm
     â”‚ Simple but powerful instance-based learning
     â”‚ Still used today!
     â”‚
1969 â”‚ Minsky & Papert
     â”‚ "Perceptrons" book
     â”‚ Showed limitations of single-layer networks
     â”‚ Caused the first "AI Winter"
     â”‚
1979 â”‚ Stanford Cart
     â”‚ Successfully navigated a room of obstacles
     â”‚ Early example of autonomous systems
     â”‚
1986 â”‚ Backpropagation
     â”‚ Rumelhart, Hinton, Williams
     â”‚ Made training deep networks possible
     â”‚ Renaissance of neural networks
     â”‚
1995 â”‚ Random Forests (Tin Kam Ho)
     â”‚ Support Vector Machines (Cortes & Vapnik)
     â”‚ Powerful algorithms still widely used
     â”‚
1997 â”‚ IBM Deep Blue beats Kasparov
     â”‚ Major milestone for game-playing AI
     â”‚ (Though more search than learning)
     â”‚
1998 â”‚ MNIST dataset released
     â”‚ Yann LeCun's LeNet-5
     â”‚ Convolutional Neural Networks for digits
     â”‚
2006 â”‚ Geoffrey Hinton
     â”‚ "Deep Learning" term popularized
     â”‚ Deep Belief Networks breakthrough
     â”‚
2009 â”‚ ImageNet dataset created
     â”‚ 14+ million labeled images
     â”‚ Enabled modern computer vision
     â”‚
2012 â”‚ AlexNet wins ImageNet
     â”‚ Deep learning revolution begins
     â”‚ Error rate dropped from 26% to 16%
     â”‚ GPU training proves essential
     â”‚
2014 â”‚ GANs introduced (Goodfellow)
     â”‚ Generative Adversarial Networks
     â”‚ Generate realistic images
     â”‚
2015 â”‚ ResNet (152 layers!)
     â”‚ Residual connections enable very deep networks
     â”‚ Superhuman performance on ImageNet
     â”‚
2016 â”‚ AlphaGo beats Lee Sedol
     â”‚ Deep reinforcement learning triumph
     â”‚ Go was considered decades away
     â”‚
2017 â”‚ "Attention Is All You Need"
     â”‚ The Transformer architecture
     â”‚ Revolutionized NLP (and later, everything)
     â”‚
2018 â”‚ BERT (Google)
     â”‚ Bidirectional transformer pretraining
     â”‚ New state-of-the-art in NLP
     â”‚
2019 â”‚ GPT-2 (OpenAI)
     â”‚ Impressive text generation
     â”‚ "Too dangerous to release" controversy
     â”‚
2020 â”‚ GPT-3 (175B parameters)
     â”‚ Few-shot learning capabilities
     â”‚ AI assistants become practical
     â”‚
2021 â”‚ DALL-E, Codex
     â”‚ Image generation from text
     â”‚ Code generation capabilities
     â”‚
2022 â”‚ ChatGPT released
     â”‚ AI goes mainstream
     â”‚ Millions of users overnight
     â”‚ The "GPT moment"
     â”‚
2023 â”‚ GPT-4, Claude, Gemini
     â”‚ Multimodal capabilities
     â”‚ Reasoning improvements
     â”‚ AI becomes a tool for everyone
     â”‚
2024 â”‚ Open source catches up
     â”‚ Llama 3, Mistral, Mixtral
     â”‚ Video generation (Sora)
     â”‚ Agent capabilities emerge
     â”‚
2025 â”‚ Reasoning models (o1, R1)
     â”‚ Reinforcement learning from verifiable rewards
     â”‚ Mixture of Experts architectures
     â”‚ AI agents in production


KEY INSIGHT: The Three Waves of AI
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Wave 1 (1950s-1970s): Symbolic AI
â”œâ”€â”€ Hand-coded rules and logic
â”œâ”€â”€ Expert systems
â””â”€â”€ Limited by human knowledge

Wave 2 (1980s-2010s): Statistical ML
â”œâ”€â”€ Learning from data
â”œâ”€â”€ SVMs, Random Forests
â””â”€â”€ Limited by feature engineering

Wave 3 (2012-present): Deep Learning
â”œâ”€â”€ End-to-end learning
â”œâ”€â”€ Minimal feature engineering
â””â”€â”€ Enabled by data + compute + algorithms
"""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1.3 WHY MACHINE LEARNING MATTERS TODAY
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
WHY MACHINE LEARNING MATTERS TODAY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

THE PERFECT STORM: Why ML Exploded in the 2010s
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Three factors converged to enable the deep learning revolution:

1. DATA EXPLOSION
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ â€¢ Internet generates exabytes of data daily                         â”‚
   â”‚ â€¢ Social media provides labeled data (likes, shares, comments)      â”‚
   â”‚ â€¢ Smartphones = sensors everywhere                                   â”‚
   â”‚ â€¢ Digitization of historical records                                â”‚
   â”‚ â€¢ IoT devices creating continuous data streams                      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

2. COMPUTE POWER
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ â€¢ GPUs: 100x faster than CPUs for matrix operations                 â”‚
   â”‚ â€¢ Cloud computing: Rent massive compute on demand                   â”‚
   â”‚ â€¢ Specialized chips: TPUs, Neural engines                           â”‚
   â”‚ â€¢ Moore's Law (until recently)                                      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3. ALGORITHMIC ADVANCES
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ â€¢ Dropout, BatchNorm: Better training stability                     â”‚
   â”‚ â€¢ ReLU: Solved vanishing gradient problem                           â”‚
   â”‚ â€¢ Residual connections: Enabled very deep networks                  â”‚
   â”‚ â€¢ Transformers: Parallelizable attention                            â”‚
   â”‚ â€¢ Better optimizers: Adam, AdamW                                    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


ML IS EVERYWHERE: Real-World Applications
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

HEALTHCARE
â”œâ”€â”€ Disease diagnosis from medical images
â”œâ”€â”€ Drug discovery and development
â”œâ”€â”€ Personalized treatment recommendations
â”œâ”€â”€ Predicting patient outcomes
â”œâ”€â”€ Analyzing genomic data
â””â”€â”€ Early detection of outbreaks

FINANCE
â”œâ”€â”€ Fraud detection
â”œâ”€â”€ Credit scoring
â”œâ”€â”€ Algorithmic trading
â”œâ”€â”€ Risk assessment
â”œâ”€â”€ Customer churn prediction
â””â”€â”€ Anti-money laundering

TECHNOLOGY
â”œâ”€â”€ Search engines (Google, Bing)
â”œâ”€â”€ Recommendation systems (Netflix, Spotify, Amazon)
â”œâ”€â”€ Virtual assistants (Siri, Alexa, Google Assistant)
â”œâ”€â”€ Email filtering
â”œâ”€â”€ Translation services
â””â”€â”€ Code completion (GitHub Copilot)

TRANSPORTATION
â”œâ”€â”€ Self-driving vehicles
â”œâ”€â”€ Route optimization
â”œâ”€â”€ Demand prediction (Uber, Lyft)
â”œâ”€â”€ Traffic prediction
â”œâ”€â”€ Predictive maintenance
â””â”€â”€ Autonomous drones

RETAIL
â”œâ”€â”€ Demand forecasting
â”œâ”€â”€ Inventory optimization
â”œâ”€â”€ Price optimization
â”œâ”€â”€ Customer segmentation
â”œâ”€â”€ Visual search
â””â”€â”€ Chatbots and customer service

ENTERTAINMENT
â”œâ”€â”€ Content recommendation
â”œâ”€â”€ Content generation
â”œâ”€â”€ Game AI
â”œâ”€â”€ Music composition
â”œâ”€â”€ Video enhancement
â””â”€â”€ Deepfakes (for better or worse)

SCIENCE
â”œâ”€â”€ Climate modeling
â”œâ”€â”€ Protein structure prediction (AlphaFold)
â”œâ”€â”€ Particle physics analysis
â”œâ”€â”€ Astronomical discovery
â”œâ”€â”€ Materials science
â””â”€â”€ Earthquake prediction

SECURITY
â”œâ”€â”€ Intrusion detection
â”œâ”€â”€ Malware classification
â”œâ”€â”€ Facial recognition
â”œâ”€â”€ Surveillance systems
â”œâ”€â”€ Biometric authentication
â””â”€â”€ Threat intelligence


THE ECONOMIC IMPACT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

According to various research reports:

â€¢ McKinsey: AI could add $13 trillion to global GDP by 2030
â€¢ PwC: AI will contribute $15.7 trillion to the global economy by 2030
â€¢ Gartner: AI will create 2.3 million jobs by 2025
â€¢ IDC: Worldwide AI spending reached $500 billion in 2024

Job market implications:
â€¢ Data Scientist consistently ranked top job
â€¢ ML Engineer salaries: $150K-$500K+ at top companies
â€¢ Demand far exceeds supply of qualified practitioners
â€¢ Every industry seeking ML expertise
"""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1.4 TYPES OF MACHINE LEARNING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
TYPES OF MACHINE LEARNING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Machine Learning algorithms are typically categorized by how they learn:

                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚    MACHINE LEARNING     â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                           â”‚                           â”‚
        â–¼                           â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SUPERVISED   â”‚         â”‚ UNSUPERVISED  â”‚         â”‚REINFORCEMENT  â”‚
â”‚   LEARNING    â”‚         â”‚   LEARNING    â”‚         â”‚   LEARNING    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                         â”‚                         â”‚
        â”‚                         â”‚                         â”‚
   Has labels              No labels                 Learn from
   (answers)               (no answers)              rewards/penalties
        â”‚                         â”‚                         â”‚
        â–¼                         â–¼                         â–¼
 â€¢ Classification          â€¢ Clustering              â€¢ Game playing
 â€¢ Regression              â€¢ Dim. reduction          â€¢ Robotics
                           â€¢ Anomaly detection       â€¢ Resource mgmt


Additional paradigms:
â”œâ”€â”€ Self-Supervised Learning: Create labels from data itself
â”œâ”€â”€ Semi-Supervised Learning: Some labels, mostly unlabeled
â””â”€â”€ Transfer Learning: Apply knowledge from one task to another
"""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1.4.1 SUPERVISED LEARNING - Detailed Explanation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
SUPERVISED LEARNING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Definition: Learning from labeled examples where both inputs (X) and 
desired outputs (y) are provided.

THE ANALOGY:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Supervised learning is like learning with a teacher who gives you:
â€¢ Practice problems (inputs)
â€¢ Answer key (labels)
You learn the patterns and can solve NEW problems.


HOW IT WORKS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Step 1: Collect labeled data
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Features (X)              â”‚  Label (y)            â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚  [3 bedrooms, 1500 sqft]   â”‚  $300,000             â”‚
        â”‚  [2 bedrooms, 1000 sqft]   â”‚  $200,000             â”‚
        â”‚  [4 bedrooms, 2000 sqft]   â”‚  $450,000             â”‚
        â”‚  ...                       â”‚  ...                  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 2: Train a model
        model.fit(X_train, y_train)
        
        The model finds patterns:
        "Each bedroom adds ~$50K, each sqft adds ~$100"

Step 3: Predict on new data
        new_house = [3 bedrooms, 1800 sqft]
        price = model.predict(new_house)  # $380,000


TWO MAIN TASKS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. CLASSIFICATION: Predict a category
   
   Examples:
   â€¢ Email â†’ Spam or Not Spam
   â€¢ Image â†’ Cat, Dog, or Bird
   â€¢ Transaction â†’ Fraudulent or Legitimate
   â€¢ Patient symptoms â†’ Disease diagnosis
   
   Output: Discrete class labels

2. REGRESSION: Predict a continuous number
   
   Examples:
   â€¢ House features â†’ Price
   â€¢ Student data â†’ Test score
   â€¢ Weather data â†’ Temperature tomorrow
   â€¢ Customer data â†’ Lifetime value
   
   Output: Continuous values


COMMON SUPERVISED LEARNING ALGORITHMS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

For Classification:
â”œâ”€â”€ Logistic Regression (simple baseline)
â”œâ”€â”€ Decision Trees
â”œâ”€â”€ Random Forests
â”œâ”€â”€ Gradient Boosting (XGBoost, LightGBM)
â”œâ”€â”€ Support Vector Machines
â”œâ”€â”€ K-Nearest Neighbors
â”œâ”€â”€ Naive Bayes
â””â”€â”€ Neural Networks

For Regression:
â”œâ”€â”€ Linear Regression (simple baseline)
â”œâ”€â”€ Polynomial Regression
â”œâ”€â”€ Decision Trees
â”œâ”€â”€ Random Forests
â”œâ”€â”€ Gradient Boosting
â”œâ”€â”€ Support Vector Regression
â””â”€â”€ Neural Networks
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# EXAMPLE 1.2: Supervised Learning - Classification
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def supervised_classification_example():
    """
    Complete supervised classification example.
    
    Task: Predict if a customer will churn (leave) based on their behavior.
    """
    import numpy as np
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    from sklearn.linear_model import LogisticRegression
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import accuracy_score, classification_report
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Step 1: Prepare labeled data
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    # Simulated customer data
    # Features: [months_as_customer, monthly_charges, total_charges, 
    #            support_tickets, login_frequency]
    np.random.seed(42)
    n_samples = 1000
    
    # Generate features
    months = np.random.randint(1, 72, n_samples)
    monthly_charges = np.random.uniform(20, 100, n_samples)
    total_charges = months * monthly_charges * np.random.uniform(0.8, 1.2, n_samples)
    support_tickets = np.random.poisson(2, n_samples)
    login_frequency = np.random.uniform(0, 30, n_samples)
    
    X = np.column_stack([months, monthly_charges, total_charges, 
                         support_tickets, login_frequency])
    
    # Generate labels based on some logic (simulating real patterns)
    # More likely to churn if: new customer, high charges, many tickets, low login
    churn_probability = (
        (months < 12).astype(float) * 0.3 +
        (monthly_charges > 70).astype(float) * 0.2 +
        (support_tickets > 3).astype(float) * 0.3 +
        (login_frequency < 5).astype(float) * 0.2
    ) / 1.0
    
    y = (np.random.random(n_samples) < churn_probability).astype(int)
    
    feature_names = ['months_customer', 'monthly_charges', 'total_charges',
                     'support_tickets', 'login_frequency']
    
    print("Dataset Overview:")
    print(f"Total samples: {n_samples}")
    print(f"Features: {feature_names}")
    print(f"Churned customers: {y.sum()} ({y.mean():.1%})")
    print(f"Retained customers: {n_samples - y.sum()} ({1-y.mean():.1%})")
    print()
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Step 2: Split data into training and testing sets
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    """
    WHY SPLIT THE DATA?
    
    We need to test our model on data it has NEVER seen before.
    This tells us how well it will perform in the real world.
    
    If we test on training data, we're just checking if the model
    memorized the answers - not if it learned the patterns!
    
    Common splits:
    - 80% train, 20% test (simple)
    - 70% train, 15% validation, 15% test (with hyperparameter tuning)
    """
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, 
        test_size=0.2,      # 20% for testing
        random_state=42,    # Reproducibility
        stratify=y          # Maintain class proportions
    )
    
    print(f"Training set: {len(X_train)} samples")
    print(f"Testing set: {len(X_test)} samples")
    print()
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Step 3: Preprocess the data
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    """
    WHY SCALE FEATURES?
    
    Our features have very different scales:
    - months_customer: 1-72
    - monthly_charges: 20-100
    - total_charges: 20-7200+
    
    Many algorithms are sensitive to scale.
    Without scaling, total_charges would dominate just because
    its numbers are bigger!
    
    StandardScaler: Transforms to mean=0, std=1
    """
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)  # Fit AND transform
    X_test_scaled = scaler.transform(X_test)        # Only transform (same params)
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Step 4: Train models
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    # Model 1: Logistic Regression (simple baseline)
    lr = LogisticRegression(random_state=42)
    lr.fit(X_train_scaled, y_train)
    
    # Model 2: Random Forest (more powerful)
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X_train_scaled, y_train)
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Step 5: Evaluate models
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    print("=" * 60)
    print("MODEL EVALUATION")
    print("=" * 60)
    
    for name, model in [("Logistic Regression", lr), ("Random Forest", rf)]:
        y_pred = model.predict(X_test_scaled)
        accuracy = accuracy_score(y_test, y_pred)
        
        print(f"\n{name}:")
        print(f"Accuracy: {accuracy:.2%}")
        print("\nDetailed Report:")
        print(classification_report(y_test, y_pred, 
                                   target_names=['Retained', 'Churned']))
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Step 6: Interpret results
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    print("\n" + "=" * 60)
    print("FEATURE IMPORTANCE (Random Forest)")
    print("=" * 60)
    
    importances = rf.feature_importances_
    for name, importance in sorted(zip(feature_names, importances), 
                                   key=lambda x: x[1], reverse=True):
        bar = "â–ˆ" * int(importance * 50)
        print(f"{name:20} {importance:.3f} {bar}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Step 7: Make predictions on new data
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    print("\n" + "=" * 60)
    print("PREDICTING FOR NEW CUSTOMERS")
    print("=" * 60)
    
    new_customers = np.array([
        [3, 80, 240, 5, 2],    # New, high charges, many tickets, low login
        [60, 50, 3000, 1, 25], # Long-time, moderate, few tickets, active
        [12, 90, 1080, 0, 10], # 1 year, high charges, no tickets, moderate
    ])
    
    new_customers_scaled = scaler.transform(new_customers)
    predictions = rf.predict(new_customers_scaled)
    probabilities = rf.predict_proba(new_customers_scaled)
    
    for i, (features, pred, proba) in enumerate(zip(new_customers, 
                                                     predictions, 
                                                     probabilities)):
        print(f"\nCustomer {i+1}: {features}")
        print(f"  Prediction: {'WILL CHURN' if pred else 'Will Stay'}")
        print(f"  Confidence: {proba[pred]:.1%}")
        
    return rf, scaler


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# EXAMPLE 1.3: Supervised Learning - Regression
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def supervised_regression_example():
    """
    Complete supervised regression example.
    
    Task: Predict house prices based on features.
    """
    import numpy as np
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    from sklearn.linear_model import LinearRegression, Ridge
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
    
    # Generate synthetic house data
    np.random.seed(42)
    n_samples = 500
    
    # Features
    bedrooms = np.random.randint(1, 6, n_samples)
    bathrooms = np.random.randint(1, 4, n_samples)
    sqft = np.random.randint(500, 4000, n_samples)
    age = np.random.randint(0, 100, n_samples)
    garage = np.random.randint(0, 3, n_samples)
    
    X = np.column_stack([bedrooms, bathrooms, sqft, age, garage])
    feature_names = ['bedrooms', 'bathrooms', 'sqft', 'age', 'garage']
    
    # Generate prices based on features (with some noise)
    base_price = 100000
    price = (
        base_price +
        bedrooms * 25000 +
        bathrooms * 15000 +
        sqft * 150 +
        -age * 1000 +
        garage * 20000 +
        np.random.normal(0, 30000, n_samples)  # Random noise
    )
    
    y = np.maximum(price, 50000)  # Minimum price of $50K
    
    print("House Price Prediction Dataset")
    print("=" * 50)
    print(f"Samples: {n_samples}")
    print(f"Features: {feature_names}")
    print(f"Price range: ${y.min():,.0f} - ${y.max():,.0f}")
    print(f"Average price: ${y.mean():,.0f}")
    print()
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Train models
    models = {
        'Linear Regression': LinearRegression(),
        'Ridge Regression': Ridge(alpha=1.0),
        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)
    }
    
    print("Model Performance")
    print("=" * 70)
    print(f"{'Model':<25} {'RMSE':>12} {'MAE':>12} {'RÂ²':>12}")
    print("-" * 70)
    
    for name, model in models.items():
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        print(f"{name:<25} ${rmse:>10,.0f} ${mae:>10,.0f} {r2:>11.3f}")
    
    # Interpret Linear Regression coefficients
    lr = models['Linear Regression']
    print("\n\nLinear Regression Coefficients (Feature Impact on Price):")
    print("-" * 60)
    
    for name, coef in zip(feature_names, lr.coef_):
        direction = "+" if coef > 0 else ""
        print(f"{name:<15}: {direction}${coef:,.0f}")
    
    print(f"{'Base price':<15}: ${lr.intercept_:,.0f}")
    
    # Predict for new houses
    print("\n\nPredicting Prices for New Houses:")
    print("-" * 60)
    
    new_houses = [
        [3, 2, 1500, 10, 2],  # 3bed, 2bath, 1500sqft, 10yr old, 2-car garage
        [4, 3, 2500, 5, 2],   # 4bed, 3bath, 2500sqft, 5yr old, 2-car garage
        [2, 1, 800, 50, 0],   # 2bed, 1bath, 800sqft, 50yr old, no garage
    ]
    
    new_houses_scaled = scaler.transform(new_houses)
    rf = models['Random Forest']
    
    for i, (features, scaled) in enumerate(zip(new_houses, new_houses_scaled)):
        pred = rf.predict([scaled])[0]
        print(f"\nHouse {i+1}:")
        print(f"  Features: {dict(zip(feature_names, features))}")
        print(f"  Predicted Price: ${pred:,.0f}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1.4.2 UNSUPERVISED LEARNING - Detailed Explanation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
UNSUPERVISED LEARNING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Definition: Learning patterns from data WITHOUT labeled examples.
The algorithm must discover structure on its own.

THE ANALOGY:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Unsupervised learning is like exploring a new city without a map:
â€¢ No one tells you what the neighborhoods are
â€¢ You discover patterns: "this area has restaurants", "this is residential"
â€¢ You group things together based on similarity


HOW IT DIFFERS FROM SUPERVISED:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Supervised:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input: Customer data + Label (churned: yes/no)                            â”‚
â”‚  Goal: Predict if NEW customers will churn                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Unsupervised:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input: Customer data only (NO labels)                                     â”‚
â”‚  Goal: Discover natural groupings of customers                             â”‚
â”‚        (maybe: "budget", "premium", "at-risk" segments)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


MAIN TASKS IN UNSUPERVISED LEARNING:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. CLUSTERING
   Finding groups of similar data points
   
   Applications:
   â€¢ Customer segmentation
   â€¢ Document grouping
   â€¢ Image compression
   â€¢ Anomaly detection
   â€¢ Gene expression analysis
   
   Algorithms:
   â€¢ K-Means
   â€¢ Hierarchical clustering
   â€¢ DBSCAN
   â€¢ Gaussian Mixture Models

2. DIMENSIONALITY REDUCTION
   Reducing the number of features while preserving information
   
   Applications:
   â€¢ Visualization of high-dimensional data
   â€¢ Noise reduction
   â€¢ Feature extraction
   â€¢ Data compression
   â€¢ Speeding up other algorithms
   
   Algorithms:
   â€¢ PCA (Principal Component Analysis)
   â€¢ t-SNE
   â€¢ UMAP
   â€¢ Autoencoders

3. ANOMALY DETECTION
   Finding unusual data points
   
   Applications:
   â€¢ Fraud detection
   â€¢ Network intrusion detection
   â€¢ Manufacturing defect detection
   â€¢ Medical diagnosis
   
   Algorithms:
   â€¢ Isolation Forest
   â€¢ One-Class SVM
   â€¢ Local Outlier Factor
   â€¢ Autoencoders

4. ASSOCIATION RULE LEARNING
   Finding relationships between variables
   
   Applications:
   â€¢ Market basket analysis ("customers who bought X also bought Y")
   â€¢ Recommendation systems
   
   Algorithms:
   â€¢ Apriori
   â€¢ FP-Growth
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# EXAMPLE 1.4: Unsupervised Learning - Clustering
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def unsupervised_clustering_example():
    """
    Complete unsupervised clustering example.
    
    Task: Segment customers into groups based on their purchasing behavior.
    No labels are provided - the algorithm discovers groups on its own!
    """
    import numpy as np
    from sklearn.cluster import KMeans, DBSCAN
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import silhouette_score
    
    # Generate synthetic customer data (no labels!)
    np.random.seed(42)
    
    # Create 4 natural clusters (but the algorithm doesn't know this!)
    
    # Cluster 1: Budget shoppers (low spending, low frequency)
    budget = np.random.normal([20, 5, 100], [5, 2, 30], (100, 3))
    
    # Cluster 2: Regular shoppers (medium spending, medium frequency)
    regular = np.random.normal([50, 15, 500], [10, 3, 100], (100, 3))
    
    # Cluster 3: Premium shoppers (high spending, high frequency)
    premium = np.random.normal([100, 30, 2000], [20, 5, 400], (100, 3))
    
    # Cluster 4: Occasional big spenders (low frequency, high per-purchase)
    occasional = np.random.normal([150, 3, 800], [30, 1, 200], (100, 3))
    
    # Combine all data
    X = np.vstack([budget, regular, premium, occasional])
    feature_names = ['avg_purchase_amount', 'monthly_visits', 'total_spend']
    
    print("Customer Segmentation (Unsupervised)")
    print("=" * 60)
    print(f"Total customers: {len(X)}")
    print(f"Features: {feature_names}")
    print("\nNote: We have NO labels - the algorithm will discover groups!\n")
    
    # Scale features (important for clustering!)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Method 1: K-Means Clustering
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    """
    K-MEANS ALGORITHM:
    
    1. Choose K (number of clusters)
    2. Randomly initialize K cluster centers
    3. Assign each point to nearest center
    4. Update centers to mean of assigned points
    5. Repeat 3-4 until convergence
    """
    
    # First, let's find the optimal number of clusters
    print("Finding optimal number of clusters...")
    print("-" * 40)
    
    silhouette_scores = []
    K_range = range(2, 10)
    
    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels = kmeans.fit_predict(X_scaled)
        score = silhouette_score(X_scaled, labels)
        silhouette_scores.append(score)
        print(f"K={k}: Silhouette Score = {score:.3f}")
    
    best_k = K_range[np.argmax(silhouette_scores)]
    print(f"\nBest K: {best_k}")
    
    # Fit final model
    kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(X_scaled)
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Analyze the discovered clusters
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    print("\n" + "=" * 60)
    print("DISCOVERED CUSTOMER SEGMENTS")
    print("=" * 60)
    
    for cluster in range(best_k):
        mask = cluster_labels == cluster
        cluster_data = X[mask]
        
        print(f"\nCluster {cluster} ({mask.sum()} customers):")
        print("-" * 40)
        
        for i, name in enumerate(feature_names):
            mean_val = cluster_data[:, i].mean()
            std_val = cluster_data[:, i].std()
            print(f"  {name:<20}: {mean_val:>8.1f} (Â±{std_val:.1f})")
        
        # Give the cluster a name based on characteristics
        avg_purchase = cluster_data[:, 0].mean()
        visits = cluster_data[:, 1].mean()
        
        if avg_purchase < 40 and visits < 10:
            segment_name = "Budget Shoppers"
        elif avg_purchase > 120:
            segment_name = "Big Spenders"
        elif visits > 20:
            segment_name = "Frequent Premium"
        else:
            segment_name = "Regular Customers"
        
        print(f"  Suggested name: {segment_name}")
    
    return kmeans, scaler, cluster_labels


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1.4.3 REINFORCEMENT LEARNING - Detailed Explanation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
REINFORCEMENT LEARNING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Definition: Learning through trial and error by receiving rewards or penalties
for actions taken in an environment.

THE ANALOGY:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Reinforcement learning is like training a dog:
â€¢ Dog performs action (sits, jumps, barks)
â€¢ You give reward (treat) or penalty (no treat, "bad dog")
â€¢ Dog learns which actions lead to rewards
â€¢ Eventually, dog learns complex behaviors


THE RL FRAMEWORK:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                                         â”‚
    â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         action (a)        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
    â”‚    â”‚         â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚                 â”‚           â”‚
    â”‚    â”‚  AGENT  â”‚                           â”‚   ENVIRONMENT   â”‚           â”‚
    â”‚    â”‚         â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                 â”‚           â”‚
    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    state (s), reward (r)  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
    â”‚                                                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Agent: The learner/decision-maker
    Environment: The world the agent interacts with
    State (s): Current situation
    Action (a): What the agent does
    Reward (r): Feedback signal (positive or negative)
    
    Goal: Learn a POLICY (strategy) that maximizes cumulative reward


KEY CONCEPTS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. POLICY (Ï€)
   A strategy that maps states to actions
   Ï€(s) â†’ a
   "When in state s, take action a"

2. VALUE FUNCTION (V)
   Expected cumulative reward from a state
   "How good is it to be in state s?"

3. Q-FUNCTION (Q)
   Expected cumulative reward from taking action a in state s
   "How good is it to take action a in state s?"

4. EXPLORATION vs EXPLOITATION
   â€¢ Exploration: Try new actions to discover better strategies
   â€¢ Exploitation: Use known good actions to maximize reward
   â€¢ Balance is crucial!


RL ALGORITHMS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Model-Free:
â”œâ”€â”€ Q-Learning: Learn Q-values for state-action pairs
â”œâ”€â”€ SARSA: On-policy variant of Q-learning
â”œâ”€â”€ Policy Gradient: Directly optimize the policy
â”œâ”€â”€ Actor-Critic: Combine value and policy methods
â””â”€â”€ PPO/TRPO: Stable policy optimization

Model-Based:
â”œâ”€â”€ Learn a model of the environment
â”œâ”€â”€ Plan using the learned model
â””â”€â”€ More sample-efficient but harder to implement


APPLICATIONS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Games:
â€¢ AlphaGo (Go)
â€¢ OpenAI Five (Dota 2)
â€¢ Atari games
â€¢ Chess, Poker

Robotics:
â€¢ Robot locomotion
â€¢ Manipulation tasks
â€¢ Autonomous vehicles

Business:
â€¢ Ad placement
â€¢ Recommendation systems
â€¢ Dynamic pricing
â€¢ Resource allocation

Science:
â€¢ Molecule design
â€¢ Experiment optimization
â€¢ Chip design (AlphaChip)
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# EXAMPLE 1.5: Simple Reinforcement Learning
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def simple_rl_example():
    """
    Simple Q-Learning example: Grid World
    
    The agent must navigate a grid to reach a goal while avoiding obstacles.
    """
    import numpy as np
    
    # Define the environment: 4x4 grid
    # 0 = empty, -1 = obstacle, 10 = goal
    
    """
    Grid layout:
    â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”
    â”‚ S  â”‚    â”‚    â”‚    â”‚
    â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤
    â”‚    â”‚ X  â”‚    â”‚    â”‚
    â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤
    â”‚    â”‚    â”‚ X  â”‚    â”‚
    â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤
    â”‚    â”‚    â”‚    â”‚ G  â”‚
    â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜
    
    S = Start (0,0)
    G = Goal (3,3)
    X = Obstacle (-10 reward)
    """
    
    GRID_SIZE = 4
    GOAL = (3, 3)
    OBSTACLES = [(1, 1), (2, 2)]
    
    # Actions: 0=up, 1=right, 2=down, 3=left
    ACTIONS = {
        0: (-1, 0),  # up
        1: (0, 1),   # right
        2: (1, 0),   # down
        3: (0, -1)   # left
    }
    ACTION_NAMES = ['â†‘', 'â†’', 'â†“', 'â†']
    
    def get_reward(state):
        """Get reward for being in a state"""
        if state == GOAL:
            return 10
        elif state in OBSTACLES:
            return -10
        else:
            return -1  # Small penalty to encourage efficiency
    
    def is_valid(state):
        """Check if state is within grid"""
        return 0 <= state[0] < GRID_SIZE and 0 <= state[1] < GRID_SIZE
    
    def take_action(state, action):
        """Take action and return new state"""
        delta = ACTIONS[action]
        new_state = (state[0] + delta[0], state[1] + delta[1])
        
        if is_valid(new_state):
            return new_state
        return state  # Stay in place if invalid move
    
    # Initialize Q-table
    # Q[state][action] = expected cumulative reward
    Q = np.zeros((GRID_SIZE, GRID_SIZE, 4))
    
    # Hyperparameters
    learning_rate = 0.1
    discount_factor = 0.99  # How much to value future rewards
    epsilon = 0.1  # Exploration rate
    episodes = 1000
    
    print("Q-Learning: Grid World Navigation")
    print("=" * 50)
    print(f"Grid size: {GRID_SIZE}x{GRID_SIZE}")
    print(f"Goal: {GOAL}")
    print(f"Obstacles: {OBSTACLES}")
    print(f"Training for {episodes} episodes...")
    print()
    
    # Training loop
    total_rewards = []
    
    for episode in range(episodes):
        state = (0, 0)  # Start position
        episode_reward = 0
        steps = 0
        max_steps = 100
        
        while state != GOAL and steps < max_steps:
            # Epsilon-greedy action selection
            if np.random.random() < epsilon:
                action = np.random.randint(4)  # Explore
            else:
                action = np.argmax(Q[state[0], state[1]])  # Exploit
            
            # Take action
            new_state = take_action(state, action)
            reward = get_reward(new_state)
            episode_reward += reward
            
            # Q-learning update
            # Q(s,a) = Q(s,a) + Î± * [r + Î³ * max(Q(s',a')) - Q(s,a)]
            old_value = Q[state[0], state[1], action]
            next_max = np.max(Q[new_state[0], new_state[1]])
            
            Q[state[0], state[1], action] = old_value + learning_rate * (
                reward + discount_factor * next_max - old_value
            )
            
            state = new_state
            steps += 1
        
        total_rewards.append(episode_reward)
        
        # Print progress
        if (episode + 1) % 200 == 0:
            avg_reward = np.mean(total_rewards[-100:])
            print(f"Episode {episode + 1}: Avg Reward (last 100) = {avg_reward:.1f}")
    
    # Display learned policy
    print("\n" + "=" * 50)
    print("LEARNED POLICY")
    print("=" * 50)
    print("\nBest action in each cell:")
    print("(S=Start, G=Goal, X=Obstacle)")
    print()
    
    for i in range(GRID_SIZE):
        row = ""
        for j in range(GRID_SIZE):
            if (i, j) == (0, 0):
                row += " S "
            elif (i, j) == GOAL:
                row += " G "
            elif (i, j) in OBSTACLES:
                row += " X "
            else:
                best_action = np.argmax(Q[i, j])
                row += f" {ACTION_NAMES[best_action]} "
        print(row)
    
    # Test the learned policy
    print("\n" + "=" * 50)
    print("TESTING LEARNED POLICY")
    print("=" * 50)
    
    state = (0, 0)
    path = [state]
    
    while state != GOAL and len(path) < 20:
        action = np.argmax(Q[state[0], state[1]])
        state = take_action(state, action)
        path.append(state)
    
    print(f"Path from start to goal: {' â†’ '.join(str(s) for s in path)}")
    print(f"Steps taken: {len(path) - 1}")
    
    return Q


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1.4.4 SELF-SUPERVISED LEARNING - Detailed Explanation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
SELF-SUPERVISED LEARNING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Definition: A form of unsupervised learning where the data provides its own
labels. The algorithm creates supervisory signals from the input data itself.

THE ANALOGY:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Self-supervised learning is like learning a language by reading books:
â€¢ No one labels each word with its meaning
â€¢ You learn patterns from context
â€¢ "The cat sat on the ___" - you can guess "mat" or "floor"
â€¢ The surrounding words supervise the learning


WHY IT'S REVOLUTIONARY:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Traditional supervised learning needs LABELED data:
â€¢ Expensive to create
â€¢ Time-consuming
â€¢ Limited in scale
â€¢ Requires domain experts

Self-supervised learning uses UNLABELED data:
â€¢ Abundant (internet has endless text, images, audio)
â€¢ Free
â€¢ Scales to billions of examples
â€¢ No manual labeling needed

This is how GPT, BERT, and most modern AI systems are trained!


COMMON PRETEXT TASKS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

For Text (Language Models):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Masked Language Modeling (BERT-style)                                    â”‚
â”‚    Input:  "The [MASK] sat on the mat"                                     â”‚
â”‚    Target: "cat"                                                           â”‚
â”‚    The model learns to predict masked words from context                   â”‚
â”‚                                                                             â”‚
â”‚ 2. Next Token Prediction (GPT-style)                                       â”‚
â”‚    Input:  "The cat sat on the"                                            â”‚
â”‚    Target: "mat"                                                           â”‚
â”‚    The model learns to predict what comes next                             â”‚
â”‚                                                                             â”‚
â”‚ 3. Next Sentence Prediction (BERT)                                         â”‚
â”‚    Given two sentences, predict if sentence B follows sentence A           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

For Images:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Contrastive Learning (SimCLR, MoCo)                                     â”‚
â”‚    - Create two augmented views of same image                              â”‚
â”‚    - Train model to recognize they're the same                             â”‚
â”‚    - Push representations of same image together                           â”‚
â”‚    - Push representations of different images apart                        â”‚
â”‚                                                                             â”‚
â”‚ 2. Masked Image Modeling (MAE)                                             â”‚
â”‚    - Mask random patches of an image                                       â”‚
â”‚    - Train model to reconstruct the masked patches                         â”‚
â”‚                                                                             â”‚
â”‚ 3. Rotation Prediction                                                      â”‚
â”‚    - Rotate image by 0Â°, 90Â°, 180Â°, or 270Â°                               â”‚
â”‚    - Train model to predict the rotation                                   â”‚
â”‚                                                                             â”‚
â”‚ 4. Jigsaw Puzzles                                                          â”‚
â”‚    - Divide image into patches and shuffle                                 â”‚
â”‚    - Train model to solve the puzzle                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


THE PRETRAIN-FINETUNE PARADIGM:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Phase 1: Pretraining (Self-supervised)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                             â”‚
â”‚  Train on MASSIVE unlabeled data                                           â”‚
â”‚  (billions of web pages, images, etc.)                                     â”‚
â”‚                                                                             â”‚
â”‚  Learn general representations of language/images/etc.                     â”‚
â”‚  This requires huge compute but only done once                             â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
Phase 2: Finetuning (Supervised)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                             â”‚
â”‚  Take pretrained model                                                      â”‚
â”‚  Train on small labeled dataset for specific task                          â”‚
â”‚                                                                             â”‚
â”‚  Examples:                                                                  â”‚
â”‚  - Sentiment classification (thousands of examples)                        â”‚
â”‚  - Named entity recognition                                                â”‚
â”‚  - Question answering                                                       â”‚
â”‚                                                                             â”‚
â”‚  Much less compute, much less data needed                                  â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


KEY INSIGHT: Why This Works
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

To predict masked words well, a model must learn:
â€¢ Grammar and syntax
â€¢ Word meanings and relationships
â€¢ World knowledge
â€¢ Reasoning abilities

These learned representations transfer to many downstream tasks!
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# EXAMPLE 1.6: Self-Supervised Learning Concept
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def self_supervised_concept_example():
    """
    Demonstrate the concept of self-supervised learning
    using a simple word prediction task.
    """
    import numpy as np
    from collections import Counter
    
    print("Self-Supervised Learning: Word Prediction")
    print("=" * 60)
    
    # Sample text corpus (imagine this is the entire internet)
    corpus = """
    The cat sat on the mat. The dog sat on the rug.
    A bird sat on the branch. The cat chased the bird.
    The dog chased the cat. The bird flew away.
    The cat sleeps on the mat. The dog sleeps on the rug.
    A happy cat purrs loudly. A happy dog wags its tail.
    """
    
    # Tokenize
    words = corpus.lower().replace('.', ' ').split()
    
    print("Training Corpus (simplified):")
    print(corpus[:200] + "...")
    print()
    
    # Build vocabulary
    vocab = list(set(words))
    word_to_idx = {w: i for i, w in enumerate(vocab)}
    idx_to_word = {i: w for i, w in enumerate(vocab)}
    
    # Create training data: (context words) â†’ (target word)
    # This is the "self-supervised" part - labels come from the data itself!
    
    """
    Self-supervision example:
    
    Sentence: "The cat sat on the mat"
    
    Training examples created automatically:
    Context: [the, sat]    â†’ Target: cat
    Context: [cat, on]     â†’ Target: sat
    Context: [sat, the]    â†’ Target: on
    ...etc...
    
    No human labeling needed!
    """
    
    window_size = 2  # Context window
    training_data = []
    
    for i, target in enumerate(words):
        # Get context words
        start = max(0, i - window_size)
        end = min(len(words), i + window_size + 1)
        context = [words[j] for j in range(start, end) if j != i]
        
        training_data.append((context, target))
    
    print("Self-Supervised Training Examples:")
    print("-" * 50)
    for context, target in training_data[:5]:
        print(f"Context: {context:<25} â†’ Target: '{target}'")
    
    # Simple "model": For each word, count what follows different contexts
    # A real model would learn embeddings, but this shows the concept
    
    context_to_predictions = {}
    for context, target in training_data:
        key = tuple(sorted(context))  # Simplified: use sorted context
        if key not in context_to_predictions:
            context_to_predictions[key] = Counter()
        context_to_predictions[key][target] += 1
    
    # Test: Predict missing word
    print("\n" + "=" * 60)
    print("TESTING LEARNED PATTERNS")
    print("=" * 60)
    
    test_cases = [
        ["the", "sat"],         # Should predict: cat/dog/bird
        ["cat", "the"],         # Should predict: chased/on
        ["on", "the"],          # Should predict: mat/rug/branch
    ]
    
    for context in test_cases:
        key = tuple(sorted(context))
        print(f"\nContext: {context}")
        print("Predicted words:")
        
        if key in context_to_predictions:
            predictions = context_to_predictions[key].most_common(3)
            for word, count in predictions:
                print(f"  '{word}': {count} occurrences")
        else:
            print("  (No direct match - real models generalize better)")
    
    print("\n" + "=" * 60)
    print("KEY INSIGHT")
    print("=" * 60)
    print("""
    The model learned patterns like:
    â€¢ "the [X] sat" â†’ X is likely an animal (cat, dog, bird)
    â€¢ "[animal] [X] on" â†’ X is likely a surface (mat, rug, branch)
    
    No one labeled this data! The structure of language itself
    provided the supervision.
    
    This is how GPT, BERT, and modern language models work,
    but at a MUCH larger scale with neural networks.
    """)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1.4.5 SEMI-SUPERVISED LEARNING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
SEMI-SUPERVISED LEARNING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Definition: Learning from a combination of labeled and unlabeled data.
Typically, you have a small amount of labeled data and a large amount
of unlabeled data.

THE ANALOGY:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Semi-supervised learning is like learning to cook:
â€¢ You have a few recipes with instructions (labeled)
â€¢ You have many photos of dishes without recipes (unlabeled)
â€¢ You use both to understand cooking patterns


WHY IT'S USEFUL:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Labeled data is expensive:
â€¢ Medical images need expert radiologists to label
â€¢ Legal documents need lawyers to annotate
â€¢ Rare languages need native speakers

Unlabeled data is cheap:
â€¢ Easy to collect
â€¢ Abundant
â€¢ No expert time needed

Semi-supervised learning: Get the best of both worlds!


COMMON APPROACHES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. SELF-TRAINING (Pseudo-labeling)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ 1. Train model on labeled data                                          â”‚
   â”‚ 2. Use model to predict labels for unlabeled data                       â”‚
   â”‚ 3. Add high-confidence predictions to training set                      â”‚
   â”‚ 4. Retrain model                                                        â”‚
   â”‚ 5. Repeat                                                               â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

2. CO-TRAINING
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ 1. Split features into two views                                        â”‚
   â”‚ 2. Train two models, each on one view                                   â”‚
   â”‚ 3. Each model labels data for the other                                 â”‚
   â”‚ 4. Models teach each other                                              â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3. CONSISTENCY REGULARIZATION
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Key idea: Model should give same prediction for augmented versions     â”‚
   â”‚                                                                         â”‚
   â”‚ For unlabeled example x:                                               â”‚
   â”‚ â€¢ Create augmented version x'                                          â”‚
   â”‚ â€¢ Enforce: model(x) â‰ˆ model(x')                                        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

4. GRAPH-BASED METHODS
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ 1. Build graph where similar examples are connected                     â”‚
   â”‚ 2. Propagate labels through the graph                                   â”‚
   â”‚ 3. Connected examples should have similar labels                        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


REAL-WORLD EXAMPLE: Medical Imaging
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Scenario:
â€¢ 1,000 labeled X-rays (expensive expert annotations)
â€¢ 100,000 unlabeled X-rays (easy to collect)

Approach:
1. Train initial model on 1,000 labeled images
2. Run model on 100,000 unlabeled images
3. For images where model is very confident (>95%), use prediction as label
4. Add these "pseudo-labeled" images to training set
5. Retrain model
6. Result: Better model using all 101,000 images
"""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1.5 THE MACHINE LEARNING WORKFLOW
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
THE MACHINE LEARNING WORKFLOW
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

A complete ML project follows this pipeline:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚DEFINE   â”‚â”€â”€â–¶â”‚COLLECT  â”‚â”€â”€â–¶â”‚PREPARE  â”‚â”€â”€â–¶â”‚TRAIN    â”‚â”€â”€â–¶â”‚EVALUATE â”‚     â”‚
â”‚   â”‚PROBLEM  â”‚   â”‚DATA     â”‚   â”‚DATA     â”‚   â”‚MODEL    â”‚   â”‚MODEL    â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                 â”‚          â”‚
â”‚                                                                 â–¼          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚MAINTAIN â”‚â—€â”€â”€â”‚MONITOR  â”‚â—€â”€â”€â”‚DEPLOY   â”‚â—€â”€â”€â”‚OPTIMIZE â”‚â—€â”€â”€â”‚TUNE     â”‚     â”‚
â”‚   â”‚         â”‚   â”‚         â”‚   â”‚         â”‚   â”‚         â”‚   â”‚PARAMS   â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


DETAILED BREAKDOWN:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. DEFINE THE PROBLEM
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Questions to answer:
   â€¢ What problem are we solving?
   â€¢ Is ML the right solution?
   â€¢ What does success look like?
   â€¢ What data do we have/need?
   â€¢ What are the constraints (time, compute, latency)?
   
   Outputs:
   â€¢ Clear problem statement
   â€¢ Success metrics
   â€¢ Project scope

2. COLLECT DATA
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Sources:
   â€¢ Internal databases
   â€¢ APIs
   â€¢ Web scraping
   â€¢ Third-party data providers
   â€¢ User-generated content
   â€¢ Sensors and IoT devices
   
   Considerations:
   â€¢ Data quality
   â€¢ Data quantity
   â€¢ Privacy and compliance
   â€¢ Representativeness
   â€¢ Cost

3. PREPARE DATA (Often 60-80% of the work!)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Tasks:
   â€¢ Exploratory Data Analysis (EDA)
   â€¢ Data cleaning (missing values, duplicates)
   â€¢ Feature engineering
   â€¢ Feature selection
   â€¢ Data transformation
   â€¢ Train/test split
   
   Common issues:
   â€¢ Missing values
   â€¢ Outliers
   â€¢ Imbalanced classes
   â€¢ Data leakage

4. TRAIN MODEL
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Steps:
   â€¢ Choose algorithm(s)
   â€¢ Set up training pipeline
   â€¢ Train initial models
   â€¢ Iterate and improve
   
   Considerations:
   â€¢ Algorithm selection
   â€¢ Training time
   â€¢ Memory requirements
   â€¢ Reproducibility

5. EVALUATE MODEL
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Metrics:
   â€¢ Classification: accuracy, precision, recall, F1, AUC
   â€¢ Regression: MSE, RMSE, MAE, RÂ²
   
   Validation:
   â€¢ Cross-validation
   â€¢ Hold-out test set
   â€¢ A/B testing
   
   Analysis:
   â€¢ Error analysis
   â€¢ Confusion matrix
   â€¢ Feature importance

6. TUNE HYPERPARAMETERS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Methods:
   â€¢ Grid search
   â€¢ Random search
   â€¢ Bayesian optimization
   â€¢ Automated ML (AutoML)
   
   Key hyperparameters vary by algorithm

7. OPTIMIZE FOR PRODUCTION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Techniques:
   â€¢ Model compression
   â€¢ Quantization
   â€¢ Pruning
   â€¢ Knowledge distillation
   â€¢ Caching
   
   Goals:
   â€¢ Reduce latency
   â€¢ Reduce memory
   â€¢ Reduce cost

8. DEPLOY MODEL
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Options:
   â€¢ REST API (Flask, FastAPI)
   â€¢ Serverless (AWS Lambda)
   â€¢ Edge deployment
   â€¢ Batch processing
   
   Infrastructure:
   â€¢ Docker containers
   â€¢ Kubernetes
   â€¢ Cloud ML services

9. MONITOR MODEL
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Track:
   â€¢ Prediction latency
   â€¢ Error rates
   â€¢ Data drift
   â€¢ Model drift
   â€¢ Business metrics
   
   Alerts:
   â€¢ Performance degradation
   â€¢ Data anomalies
   â€¢ System errors

10. MAINTAIN MODEL
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Tasks:
    â€¢ Regular retraining
    â€¢ Feature updates
    â€¢ Bug fixes
    â€¢ Version management
    
    Triggers for retraining:
    â€¢ Performance degradation
    â€¢ New data available
    â€¢ Business requirements change
"""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# EXAMPLE 1.7: Complete ML Workflow
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def complete_ml_workflow_example():
    """
    Demonstrates a complete ML workflow from problem definition to evaluation.
    
    Problem: Predict customer satisfaction based on interaction data.
    """
    import numpy as np
    import pandas as pd
    from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
    from sklearn.preprocessing import StandardScaler, LabelEncoder
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import classification_report, confusion_matrix
    import warnings
    warnings.filterwarnings('ignore')
    
    print("=" * 70)
    print("COMPLETE ML WORKFLOW EXAMPLE")
    print("=" * 70)
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # STEP 1: DEFINE THE PROBLEM
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    print("\n" + "â”€" * 70)
    print("STEP 1: DEFINE THE PROBLEM")
    print("â”€" * 70)
    print("""
    Problem: Predict if a customer will be satisfied or dissatisfied
             based on their support interaction.
    
    Business value: 
    - Identify at-risk customers before they leave
    - Improve customer service processes
    - Reduce churn rate
    
    Success metric: F1 score > 0.80
    Constraint: Must run in < 100ms for real-time prediction
    """)
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # STEP 2: COLLECT DATA (simulated)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    print("\n" + "â”€" * 70)
    print("STEP 2: COLLECT DATA")
    print("â”€" * 70)
    
    np.random.seed(42)
    n_samples = 2000
    
    # Generate synthetic customer interaction data
    data = {
        'wait_time_minutes': np.random.exponential(5, n_samples),
        'interaction_duration': np.random.normal(15, 5, n_samples),
        'num_transfers': np.random.poisson(0.5, n_samples),
        'agent_experience_years': np.random.uniform(0, 10, n_samples),
        'issue_complexity': np.random.choice(['low', 'medium', 'high'], n_samples),
        'channel': np.random.choice(['phone', 'chat', 'email'], n_samples),
        'time_of_day': np.random.choice(['morning', 'afternoon', 'evening'], n_samples),
        'first_contact_resolution': np.random.choice([0, 1], n_samples, p=[0.3, 0.7]),
    }
    
    # Generate satisfaction based on features (realistic patterns)
    satisfaction_score = (
        - data['wait_time_minutes'] * 0.1
        - data['num_transfers'] * 0.3
        + data['agent_experience_years'] * 0.05
        + data['first_contact_resolution'] * 0.5
        - (np.array(data['issue_complexity']) == 'high').astype(float) * 0.2
        + np.random.normal(0, 0.3, n_samples)
    )
    
    data['satisfied'] = (satisfaction_score > np.median(satisfaction_score)).astype(int)
    
    df = pd.DataFrame(data)
    
    print(f"Dataset shape: {df.shape}")
    print(f"\nFeatures:")
    for col in df.columns[:-1]:
        print(f"  - {col}")
    print(f"\nTarget: satisfied (0=No, 1=Yes)")
    print(f"Class distribution: {dict(df['satisfied'].value_counts())}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # STEP 3: PREPARE DATA (EDA + Preprocessing)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    print("\n" + "â”€" * 70)
    print("STEP 3: PREPARE DATA")
    print("â”€" * 70)
    
    # 3a. Exploratory Data Analysis
    print("\n3a. Exploratory Data Analysis")
    print("-" * 40)
    
    print("\nNumerical features summary:")
    print(df.describe().round(2))
    
    print("\nMissing values:")
    print(df.isnull().sum())
    
    print("\nCorrelations with target:")
    numerical_cols = df.select_dtypes(include=[np.number]).columns
    for col in numerical_cols:
        if col != 'satisfied':
            corr = df[col].corr(df['satisfied'])
            print(f"  {col}: {corr:.3f}")
    
    # 3b. Feature Engineering
    print("\n3b. Feature Engineering")
    print("-" * 40)
    
    # Create new features
    df['wait_per_transfer'] = df['wait_time_minutes'] / (df['num_transfers'] + 1)
    df['efficiency_score'] = df['first_contact_resolution'] * df['agent_experience_years']
    
    print("Created features: wait_per_transfer, efficiency_score")
    
    # 3c. Encode categorical variables
    print("\n3c. Encoding Categorical Variables")
    print("-" * 40)
    
    # One-hot encoding for nominal categories
    df_encoded = pd.get_dummies(df, columns=['channel', 'time_of_day'], drop_first=True)
    
    # Label encoding for ordinal category
    complexity_map = {'low': 0, 'medium': 1, 'high': 2}
    df_encoded['issue_complexity'] = df_encoded['issue_complexity'].map(complexity_map)
    
    print(f"Columns after encoding: {list(df_encoded.columns)}")
    
    # 3d. Split data
    print("\n3d. Train/Test Split")
    print("-" * 40)
    
    X = df_encoded.drop('satisfied', axis=1)
    y = df_encoded['satisfied']
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"Training set: {len(X_train)} samples")
    print(f"Test set: {len(X_test)} samples")
    
    # 3e. Scale features
    print("\n3e. Feature Scaling")
    print("-" * 40)
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print("Applied StandardScaler (mean=0, std=1)")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # STEP 4: TRAIN MODEL
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    print("\n" + "â”€" * 70)
    print("STEP 4: TRAIN MODEL")
    print("â”€" * 70)
    
    # Train baseline model
    print("\nTraining Random Forest classifier...")
    
    rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    rf.fit(X_train_scaled, y_train)
    
    # Cross-validation
    cv_scores = cross_val_score(rf, X_train_scaled, y_train, cv=5, scoring='f1')
    print(f"\n5-Fold Cross-validation F1 scores: {cv_scores.round(3)}")
    print(f"Mean CV F1: {cv_scores.mean():.3f} (+/- {cv_scores.std()*2:.3f})")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # STEP 5: EVALUATE MODEL
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    print("\n" + "â”€" * 70)
    print("STEP 5: EVALUATE MODEL")
    print("â”€" * 70)
    
    y_pred = rf.predict(X_test_scaled)
    
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=['Dissatisfied', 'Satisfied']))
    
    print("\nConfusion Matrix:")
    cm = confusion_matrix(y_test, y_pred)
    print(f"                 Predicted")
    print(f"               Dis    Sat")
    print(f"Actual Dis   {cm[0,0]:4d}   {cm[0,1]:4d}")
    print(f"       Sat   {cm[1,0]:4d}   {cm[1,1]:4d}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # STEP 6: TUNE HYPERPARAMETERS
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    print("\n" + "â”€" * 70)
    print("STEP 6: TUNE HYPERPARAMETERS")
    print("â”€" * 70)
    
    print("\nRunning Grid Search (this may take a moment)...")
    
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [5, 10, 15, None],
        'min_samples_split': [2, 5, 10],
    }
    
    # Use smaller grid for demo
    small_param_grid = {
        'n_estimators': [50, 100],
        'max_depth': [5, 10],
        'min_samples_split': [2, 5],
    }
    
    grid_search = GridSearchCV(
        RandomForestClassifier(random_state=42),
        small_param_grid,
        cv=3,
        scoring='f1',
        n_jobs=-1
    )
    grid_search.fit(X_train_scaled, y_train)
    
    print(f"\nBest parameters: {grid_search.best_params_}")
    print(f"Best CV F1 score: {grid_search.best_score_:.3f}")
    
    # Train final model with best parameters
    best_model = grid_search.best_estimator_
    y_pred_best = best_model.predict(X_test_scaled)
    
    print(f"\nFinal model test F1: {classification_report(y_test, y_pred_best, output_dict=True)['weighted avg']['f1-score']:.3f}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # STEP 7: INTERPRET RESULTS
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    print("\n" + "â”€" * 70)
    print("STEP 7: INTERPRET RESULTS")
    print("â”€" * 70)
    
    print("\nFeature Importance:")
    importance_df = pd.DataFrame({
        'feature': X.columns,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    for _, row in importance_df.head(10).iterrows():
        bar = "â–ˆ" * int(row['importance'] * 50)
        print(f"  {row['feature']:<25} {row['importance']:.3f} {bar}")
    
    print("\n" + "â”€" * 70)
    print("SUMMARY")
    print("â”€" * 70)
    print(f"""
    Model Performance:
    - Achieved F1 score: {grid_search.best_score_:.3f}
    - Target F1 score: 0.80
    - Status: {'âœ“ PASSED' if grid_search.best_score_ > 0.80 else 'âœ— Needs improvement'}
    
    Key Insights:
    - First contact resolution is the most important predictor
    - Wait time and number of transfers negatively impact satisfaction
    - Agent experience helps improve outcomes
    
    Next Steps:
    1. Deploy model to production
    2. Set up monitoring for data drift
    3. Schedule monthly retraining
    4. A/B test against current system
    """)
    
    return best_model, scaler


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1.6 WHEN TO USE (AND NOT USE) MACHINE LEARNING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
WHEN TO USE (AND NOT USE) MACHINE LEARNING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ML is powerful, but it's not always the right solution.

WHEN TO USE ML:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âœ“ You have a clear learning task
  â€¢ Classification, regression, clustering, etc.
  â€¢ Well-defined inputs and outputs

âœ“ The problem is too complex for explicit rules
  â€¢ Image recognition (millions of pixel combinations)
  â€¢ Natural language (infinite valid sentences)
  â€¢ Complex pattern recognition

âœ“ You have sufficient data
  â€¢ Enough examples to learn patterns
  â€¢ Representative of real-world scenarios
  â€¢ Data quality is acceptable

âœ“ The patterns are learnable
  â€¢ There's a relationship between inputs and outputs
  â€¢ Patterns are somewhat consistent
  â€¢ Not purely random

âœ“ You need to handle variability
  â€¢ Many edge cases to handle
  â€¢ Rules would be too numerous
  â€¢ New patterns emerge over time

âœ“ You need to scale
  â€¢ Can't manually process all cases
  â€¢ Need automated decisions
  â€¢ Volume too high for humans


WHEN NOT TO USE ML:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âœ— Simple rules work fine
  â€¢ "If age < 18, deny access"
  â€¢ "If price > budget, don't show item"
  â€¢ Clear, simple logic

âœ— You don't have enough data
  â€¢ ML needs examples to learn
  â€¢ Rule of thumb: hundreds to thousands for basic tasks
  â€¢ Millions for deep learning from scratch

âœ— The problem isn't predictable
  â€¢ Purely random events
  â€¢ No patterns to learn
  â€¢ Fundamental uncertainty

âœ— You need perfect accuracy
  â€¢ ML models make mistakes
  â€¢ Some domains require 100% correctness
  â€¢ Consider human-in-the-loop

âœ— You can't explain decisions
  â€¢ Some regulated domains require explainability
  â€¢ "Why was my loan denied?"
  â€¢ Consider interpretable models

âœ— The cost of errors is too high
  â€¢ Medical diagnosis (without human review)
  â€¢ Autonomous weapons
  â€¢ Irreversible decisions

âœ— Simpler solutions exist
  â€¢ Don't use a neural network for averaging numbers
  â€¢ Simple heuristics often work well
  â€¢ Complexity has costs


DECISION FRAMEWORK:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Ask these questions:

1. Is there a pattern to learn?
   NO  â†’ Don't use ML
   YES â†’ Continue

2. Can you get enough quality data?
   NO  â†’ Consider if you can simplify, or use rules
   YES â†’ Continue

3. Can a simpler solution work?
   YES â†’ Use the simpler solution
   NO  â†’ Continue

4. Can you tolerate some errors?
   NO  â†’ Reconsider, add human oversight
   YES â†’ Continue

5. Do you have the infrastructure to deploy and maintain?
   NO  â†’ Build infrastructure first
   YES â†’ Use ML!


REAL-WORLD EXAMPLES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

USE ML:
â€¢ Spam filtering (complex patterns, lots of data, errors acceptable)
â€¢ Product recommendations (complex, lots of data, scales well)
â€¢ Fraud detection (subtle patterns, lots of data, errors acceptable)
â€¢ Voice recognition (impossible to write rules)

DON'T USE ML:
â€¢ Calculating sales tax (simple formula)
â€¢ Password validation (regex works fine)
â€¢ Sorting a list (algorithms exist)
â€¢ Calculating age from birthdate (simple math)

MAYBE USE ML:
â€¢ Credit decisions (regulated, but patterns exist)
â€¢ Medical diagnosis (high stakes, but AI can assist humans)
â€¢ Self-driving cars (complex, but safety-critical)
"""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1.7 SETTING UP YOUR ML ENVIRONMENT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
SETTING UP YOUR ML ENVIRONMENT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

This section covers how to set up a professional ML development environment.


OPTION 1: LOCAL SETUP (Recommended for learning)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Step 1: Install Python
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Download Python 3.9+ from python.org
OR use Anaconda (recommended for beginners)

# Check Python version
python --version


Step 2: Create Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Using venv (built-in)
python -m venv ml_env
source ml_env/bin/activate  # Linux/Mac
ml_env\\Scripts\\activate   # Windows

# Using conda (if you installed Anaconda)
conda create -n ml_env python=3.10
conda activate ml_env


Step 3: Install Core Libraries
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Essential ML libraries
pip install numpy pandas scikit-learn matplotlib seaborn

# Deep learning (choose based on your needs)
pip install torch torchvision  # PyTorch
pip install tensorflow          # TensorFlow

# Additional useful libraries
pip install jupyter notebook
pip install xgboost lightgbm catboost
pip install transformers datasets
pip install plotly
pip install optuna  # Hyperparameter tuning


Step 4: Verify Installation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python -c "import numpy; print(f'NumPy: {numpy.__version__}')"
python -c "import pandas; print(f'Pandas: {pandas.__version__}')"
python -c "import sklearn; print(f'Scikit-learn: {sklearn.__version__}')"
python -c "import torch; print(f'PyTorch: {torch.__version__}')"


OPTION 2: CLOUD NOTEBOOKS (Quick start, no setup)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Google Colab (Free, includes GPU!)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Go to colab.research.google.com
â€¢ Sign in with Google account
â€¢ Create new notebook
â€¢ Libraries pre-installed
â€¢ Free GPU/TPU access

Kaggle Notebooks
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Go to kaggle.com
â€¢ Create account
â€¢ Start new notebook
â€¢ Free GPU, many datasets

Amazon SageMaker Studio Lab (Free)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Go to studiolab.sagemaker.aws
â€¢ Request free account
â€¢ Full Jupyter environment


OPTION 3: DOCKER (Reproducible environments)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Dockerfile for ML environment
# Save as Dockerfile

FROM python:3.10-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \\
    build-essential \\
    && rm -rf /var/lib/apt/lists/*

# Install Python packages
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Start Jupyter
CMD ["jupyter", "notebook", "--ip=0.0.0.0", "--port=8888", "--allow-root"]


# requirements.txt
numpy==1.24.0
pandas==2.0.0
scikit-learn==1.3.0
matplotlib==3.7.0
seaborn==0.12.0
jupyter==1.0.0
torch==2.0.0
xgboost==1.7.0


# Build and run
docker build -t ml-env .
docker run -p 8888:8888 -v $(pwd):/app ml-env


RECOMMENDED PROJECT STRUCTURE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ml_project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Original, immutable data
â”‚   â”œâ”€â”€ processed/        # Cleaned, transformed data
â”‚   â””â”€â”€ external/         # Data from external sources
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda.ipynb      # Exploratory analysis
â”‚   â”œâ”€â”€ 02_preprocessing.ipynb
â”‚   â””â”€â”€ 03_modeling.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ build_features.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â””â”€â”€ predict.py
â”‚   â””â”€â”€ visualization/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ plots.py
â”œâ”€â”€ models/               # Saved model files
â”œâ”€â”€ reports/             
â”‚   â””â”€â”€ figures/          # Generated graphics
â”œâ”€â”€ tests/                # Unit tests
â”œâ”€â”€ requirements.txt      # Dependencies
â”œâ”€â”€ setup.py             # Make project installable
â”œâ”€â”€ config.yaml          # Configuration
â””â”€â”€ README.md


IDE RECOMMENDATIONS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

For Beginners:
â€¢ Jupyter Notebook/Lab - Interactive, great for exploration
â€¢ VS Code - Free, excellent Python support, integrated terminal

For Professionals:
â€¢ VS Code + extensions - Most popular, highly customizable
â€¢ PyCharm Professional - Powerful IDE, great debugging

Useful VS Code Extensions:
â€¢ Python (Microsoft)
â€¢ Pylance
â€¢ Jupyter
â€¢ Python Docstring Generator
â€¢ GitLens
â€¢ Error Lens


GPU SETUP (For Deep Learning):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

NVIDIA GPU on Linux/Windows:
1. Install NVIDIA driver from nvidia.com
2. Install CUDA toolkit (version compatible with PyTorch/TensorFlow)
3. Install cuDNN
4. Install PyTorch with CUDA support:
   pip install torch --index-url https://download.pytorch.org/whl/cu118

Verify GPU:
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}")

Mac with Apple Silicon:
â€¢ Use PyTorch with MPS (Metal Performance Shaders)
â€¢ pip install torch torchvision torchaudio
â€¢ device = "mps" if torch.backends.mps.is_available() else "cpu"
"""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1.8 CHAPTER 1 EXERCISES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
CHAPTER 1 EXERCISES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EXERCISE 1.1: Identify ML Type
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
For each scenario, identify the type of ML (supervised, unsupervised, 
reinforcement):

a) Grouping similar news articles together
b) Predicting house prices from features
c) Teaching a robot to walk
d) Detecting fraudulent transactions (with labeled fraud data)
e) Finding customer segments from purchase history
f) Recommending movies based on what similar users liked
g) Playing chess against itself to improve

Answers at end of exercise section.


EXERCISE 1.2: Regression vs Classification
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Determine if each task is regression or classification:

a) Predicting tomorrow's temperature
b) Determining if an email is spam
c) Estimating how long a delivery will take
d) Identifying which digit (0-9) is in an image
e) Predicting stock price change (up/down)
f) Predicting exact stock price
g) Determining cancer type from biopsy

Answers at end of exercise section.


EXERCISE 1.3: ML Pipeline Design
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
You're building a model to predict if a customer will churn (leave).

Answer these questions:
a) What features might you collect?
b) What target variable would you use?
c) Is this classification or regression?
d) What metrics would you use to evaluate?
e) What would be the business impact of false positives vs false negatives?


EXERCISE 1.4: Coding Challenge
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Implement a simple classifier that predicts if a fruit is an apple or orange
based on weight and texture (scale 1-10, where 1=smooth, 10=bumpy).

Training data:
Apple:  weight ~150g, texture ~3
Orange: weight ~130g, texture ~8

Hint: You can use sklearn's KNeighborsClassifier or LogisticRegression.


EXERCISE 1.5: Critical Thinking
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
For each scenario, explain why ML might NOT be the best solution:

a) Calculating the area of a rectangle given length and width
b) Determining if a number is even or odd
c) Predicting lottery numbers
d) Converting temperatures between Celsius and Fahrenheit


EXERCISE 1.6: Research Task
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Pick one of these topics and write a short summary (200-300 words):

a) The ImageNet competition and its impact on deep learning
b) How AlphaGo defeated world champion Lee Sedol
c) The development of GPT models from GPT-1 to GPT-4
d) The difference between narrow AI and general AI


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ANSWERS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Exercise 1.1 Answers:
a) Unsupervised (clustering)
b) Supervised (regression)
c) Reinforcement learning
d) Supervised (classification)
e) Unsupervised (clustering)
f) Supervised (collaborative filtering uses ratings/labels)
g) Reinforcement learning

Exercise 1.2 Answers:
a) Regression (continuous temperature)
b) Classification (spam or not spam)
c) Regression (continuous time)
d) Classification (one of 10 classes)
e) Classification (up or down)
f) Regression (continuous price)
g) Classification (one of multiple types)
"""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# EXERCISE 1.4 SOLUTION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def exercise_1_4_solution():
    """Solution to Exercise 1.4: Fruit classifier"""
    import numpy as np
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.linear_model import LogisticRegression
    
    # Training data
    # [weight (g), texture (1-10)]
    X_train = np.array([
        [150, 3],   # Apple
        [155, 2],   # Apple
        [148, 4],   # Apple
        [145, 3],   # Apple
        [160, 2],   # Apple
        [130, 8],   # Orange
        [125, 9],   # Orange
        [135, 7],   # Orange
        [128, 8],   # Orange
        [132, 9],   # Orange
    ])
    
    y_train = ['apple', 'apple', 'apple', 'apple', 'apple',
               'orange', 'orange', 'orange', 'orange', 'orange']
    
    # Train KNN classifier
    knn = KNeighborsClassifier(n_neighbors=3)
    knn.fit(X_train, y_train)
    
    # Train Logistic Regression
    lr = LogisticRegression()
    lr.fit(X_train, y_train)
    
    # Test on new fruits
    test_fruits = np.array([
        [152, 3],   # Should be apple
        [128, 8],   # Should be orange
        [140, 5],   # Borderline case
    ])
    
    print("Fruit Classification Results")
    print("=" * 50)
    print("\nTest fruits: [weight, texture]")
    
    for i, fruit in enumerate(test_fruits):
        knn_pred = knn.predict([fruit])[0]
        lr_pred = lr.predict([fruit])[0]
        
        print(f"\nFruit {i+1}: weight={fruit[0]}g, texture={fruit[1]}")
        print(f"  KNN prediction: {knn_pred}")
        print(f"  Logistic Regression prediction: {lr_pred}")
    
    return knn, lr


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1.9 CHAPTER 1 SUMMARY
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
CHAPTER 1 SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

KEY TAKEAWAYS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. MACHINE LEARNING is teaching computers to learn patterns from data
   instead of explicitly programming rules.

2. THREE MAIN TYPES:
   â€¢ Supervised: Learn from labeled data (has answers)
   â€¢ Unsupervised: Find patterns in unlabeled data (no answers)
   â€¢ Reinforcement: Learn through trial and error (rewards/penalties)

3. SUPERVISED LEARNING has two main tasks:
   â€¢ Classification: Predict categories (spam/not spam)
   â€¢ Regression: Predict continuous values (price, temperature)

4. THE ML PIPELINE:
   Define Problem â†’ Collect Data â†’ Prepare Data â†’ Train â†’ Evaluate â†’ 
   Tune â†’ Deploy â†’ Monitor â†’ Maintain

5. DATA PREPARATION is often 60-80% of the work!

6. ML IS NOT ALWAYS THE ANSWER:
   â€¢ Use simple rules when they work
   â€¢ Need sufficient data
   â€¢ Must tolerate some errors
   â€¢ Consider explainability requirements

7. MODERN ML is driven by:
   â€¢ Big data
   â€¢ Powerful GPUs
   â€¢ Algorithmic advances (transformers, etc.)


VOCABULARY:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Features (X): Input variables used to make predictions
Labels (y): Target variables we're trying to predict
Training: The process of learning patterns from data
Model: The learned function that maps inputs to outputs
Inference: Using a trained model to make predictions
Overfitting: Model memorizes training data, fails on new data
Underfitting: Model is too simple to capture patterns
Supervised: Learning with labeled data
Unsupervised: Learning without labels
Reinforcement: Learning through rewards and penalties


NEXT CHAPTER PREVIEW:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

In Chapter 2, we'll cover the mathematical foundations of ML:
â€¢ Linear algebra (vectors, matrices)
â€¢ Calculus (derivatives, gradients)
â€¢ Probability and statistics
â€¢ Information theory

This math forms the backbone of all ML algorithms!
"""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# End of Chapter 1
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
#                                                                               
#   CHAPTER 2: MATHEMATICS FOR MACHINE LEARNING                                  
#                                                                               
# â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ

"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘   "The book of nature is written in the language of mathematics."             â•‘
â•‘                                              â€” Galileo Galilei                â•‘
â•‘                                                                               â•‘
â•‘   Don't worry if math isn't your strongest subject. We'll build up            â•‘
â•‘   intuitively, with code examples you can run to see the concepts in action.  â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import numpy as np
from scipy import stats

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2.1 LINEAR ALGEBRA ESSENTIALS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
LINEAR ALGEBRA ESSENTIALS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Linear algebra is the foundation of machine learning. Every ML algorithm
can be expressed in terms of vectors and matrices.

WHY LINEAR ALGEBRA?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Data is represented as vectors and matrices
â€¢ Model parameters are vectors
â€¢ Predictions are matrix multiplications
â€¢ Optimizations involve gradients (vectors)
â€¢ GPUs are optimized for matrix operations


SCALARS, VECTORS, AND MATRICES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SCALARS: A single number (e.g., temperature, learning rate)

VECTORS: An ordered list of numbers (1D array)
         Example: A point in 2D: [3, 4]
         
         Visual:
               â”Œâ”€â”€â”€â”
               â”‚ 3 â”‚
         x =   â”‚ 1 â”‚    This is a 4-dimensional vector
               â”‚ 4 â”‚
               â”‚ 2 â”‚
               â””â”€â”€â”€â”˜

MATRICES: A 2D array of numbers
          Example: Dataset rows are samples, columns are features
          
          Visual:
               â”Œ             â”
               â”‚ 1   2   3   â”‚
         A =   â”‚ 4   5   6   â”‚    This is a 3Ã—3 matrix
               â”‚ 7   8   9   â”‚
               â””             â”˜

TENSORS: Generalization to any number of dimensions
         â€¢ Scalar: 0D tensor
         â€¢ Vector: 1D tensor
         â€¢ Matrix: 2D tensor
         â€¢ 3D tensor: Stack of matrices (e.g., color image)
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# EXAMPLE 2.1: Creating Scalars, Vectors, and Matrices
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def example_scalars_vectors_matrices():
    """Demonstrate creating and working with basic linear algebra objects."""
    
    print("SCALARS, VECTORS, AND MATRICES")
    print("=" * 60)
    
    # SCALARS
    print("\n1. SCALARS")
    learning_rate = 0.001
    print(f"Learning rate: {learning_rate}")
    
    # VECTORS
    print("\n2. VECTORS")
    v1 = np.array([1, 2, 3, 4, 5])
    v2 = np.zeros(5)           # [0, 0, 0, 0, 0]
    v3 = np.ones(5)            # [1, 1, 1, 1, 1]
    v4 = np.random.randn(5)    # Random from standard normal
    
    print(f"v1 = {v1}")
    print(f"v1 shape: {v1.shape}")
    print(f"v1[0] = {v1[0]}, v1[-1] = {v1[-1]}")
    
    # MATRICES
    print("\n3. MATRICES")
    A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
    I = np.eye(4)              # 4x4 identity matrix
    
    print(f"Matrix A:\n{A}")
    print(f"A shape: {A.shape}")
    print(f"A[0, 0] = {A[0, 0]}, A[1, 2] = {A[1, 2]}")
    
    # TENSORS
    print("\n4. TENSORS")
    image = np.random.randint(0, 256, size=(100, 100, 3))  # RGB image
    batch = np.random.randn(32, 100, 100, 3)  # Batch of images
    
    print(f"Image tensor shape: {image.shape}")
    print(f"Batch tensor shape: {batch.shape}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# EXAMPLE 2.2: Essential Matrix Operations
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def example_matrix_operations():
    """Demonstrate essential matrix operations for ML."""
    
    print("MATRIX OPERATIONS")
    print("=" * 60)
    
    A = np.array([[1, 2], [3, 4]])
    B = np.array([[5, 6], [7, 8]])
    x = np.array([1, 2])
    
    print(f"Matrix A:\n{A}")
    print(f"Matrix B:\n{B}")
    
    # ELEMENT-WISE OPERATIONS
    print("\n1. ELEMENT-WISE OPERATIONS")
    print(f"A + B =\n{A + B}")
    print(f"A * B (element-wise) =\n{A * B}")
    
    # MATRIX MULTIPLICATION
    print("\n2. MATRIX MULTIPLICATION")
    """
    For matrices A (mÃ—n) and B (nÃ—p):
    C = A @ B has shape (mÃ—p)
    
    Rule: Inner dimensions must match!
    """
    C = A @ B  # or np.dot(A, B)
    print(f"A @ B =\n{C}")
    
    # Matrix-Vector multiplication
    print(f"\nA @ x = {A @ x}")
    
    # DOT PRODUCT
    print("\n3. DOT PRODUCT")
    a = np.array([1, 2, 3])
    b = np.array([4, 5, 6])
    print(f"a Â· b = {np.dot(a, b)}")  # 1*4 + 2*5 + 3*6 = 32
    
    # TRANSPOSE
    print("\n4. TRANSPOSE")
    M = np.array([[1, 2, 3], [4, 5, 6]])
    print(f"M:\n{M}")
    print(f"M transpose:\n{M.T}")
    
    # NORMS
    print("\n5. NORMS")
    v = np.array([3, 4])
    print(f"L1 norm: {np.linalg.norm(v, ord=1)}")  # |3| + |4| = 7
    print(f"L2 norm: {np.linalg.norm(v, ord=2)}")  # âˆš(9+16) = 5
    
    # INVERSE
    print("\n6. INVERSE")
    A_inv = np.linalg.inv(A)
    print(f"A inverse:\n{A_inv}")
    print(f"A @ Aâ»Â¹ =\n{(A @ A_inv).round(10)}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# EIGENVALUES AND EIGENVECTORS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
EIGENVALUES AND EIGENVECTORS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

For a square matrix A, an eigenvector v and eigenvalue Î» satisfy:

    A @ v = Î» Ã— v

In words: When matrix A transforms vector v, it only SCALES v (by Î»).

WHY THEY MATTER IN ML:
â€¢ PCA: Eigenvectors of covariance matrix are principal components
â€¢ Spectral Clustering: Uses eigenvectors of graph Laplacian
â€¢ Understanding matrix transformations
"""

def example_eigenvalues():
    """Demonstrate eigenvalues and eigenvectors."""
    
    print("EIGENVALUES AND EIGENVECTORS")
    print("=" * 60)
    
    A = np.array([[4, 2], [1, 3]])
    eigenvalues, eigenvectors = np.linalg.eig(A)
    
    print(f"Matrix A:\n{A}")
    print(f"\nEigenvalues: {eigenvalues}")
    print(f"\nEigenvectors (as columns):\n{eigenvectors}")
    
    # Verify: A @ v = Î» Ã— v
    print("\nVerification:")
    for i in range(len(eigenvalues)):
        v = eigenvectors[:, i]
        lam = eigenvalues[i]
        print(f"A @ v{i+1} = {(A @ v).round(4)}")
        print(f"Î»{i+1} Ã— v{i+1} = {(lam * v).round(4)}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SINGULAR VALUE DECOMPOSITION (SVD)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
SVD decomposes any matrix A into: A = U @ Î£ @ V^T

Uses in ML:
â€¢ PCA (dimensionality reduction)
â€¢ Matrix completion (recommendations)
â€¢ Image compression
"""

def example_svd():
    """Demonstrate SVD and low-rank approximation."""
    
    print("SINGULAR VALUE DECOMPOSITION")
    print("=" * 60)
    
    A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])
    U, s, Vt = np.linalg.svd(A, full_matrices=False)
    
    print(f"Original matrix A (4Ã—3):\n{A}")
    print(f"\nSingular values: {s.round(3)}")
    
    # Low-rank approximation
    k = 1
    A_approx = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]
    print(f"\nRank-{k} approximation:\n{A_approx.round(2)}")
    print(f"Approximation error: {np.linalg.norm(A - A_approx):.2f}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2.2 CALCULUS FOR MACHINE LEARNING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
CALCULUS FOR MACHINE LEARNING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

KEY INSIGHT: To minimize a loss function, we need to know which direction
to move parameters. Derivatives (gradients) tell us the direction of 
steepest increase, so we move in the OPPOSITE direction.


DERIVATIVES AND GRADIENTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

DERIVATIVE (Single Variable):
The derivative f'(x) tells you the rate of change of f at point x.

Common derivatives:
â€¢ d/dx (x^n) = n Ã— x^(n-1)
â€¢ d/dx (e^x) = e^x
â€¢ d/dx (ln(x)) = 1/x

GRADIENT (Multiple Variables):
When f depends on multiple variables:
âˆ‡f(x) = [âˆ‚f/âˆ‚xâ‚, âˆ‚f/âˆ‚xâ‚‚, ..., âˆ‚f/âˆ‚xâ‚™]

The gradient points in the direction of steepest increase.
"""

def example_derivatives_gradients():
    """Demonstrate derivatives and gradients."""
    
    print("DERIVATIVES AND GRADIENTS")
    print("=" * 60)
    
    # Numerical derivative
    def f(x):
        return x ** 2
    
    def numerical_derivative(func, x, h=1e-7):
        return (func(x + h) - func(x - h)) / (2 * h)
    
    x = 3.0
    print(f"f(x) = xÂ²")
    print(f"At x = {x}:")
    print(f"  Numerical derivative: {numerical_derivative(f, x):.6f}")
    print(f"  Analytical (2x): {2*x:.6f}")
    
    # Gradient
    def g(params):
        x, y = params
        return x**2 + y**2
    
    def numerical_gradient(func, params, h=1e-7):
        gradient = np.zeros_like(params, dtype=float)
        for i in range(len(params)):
            params_plus = params.copy()
            params_minus = params.copy()
            params_plus[i] += h
            params_minus[i] -= h
            gradient[i] = (func(params_plus) - func(params_minus)) / (2 * h)
        return gradient
    
    point = np.array([3.0, 4.0])
    grad = numerical_gradient(g, point)
    print(f"\ng(x,y) = xÂ² + yÂ²")
    print(f"At point {point}:")
    print(f"  Gradient: {grad}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# THE CHAIN RULE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
THE CHAIN RULE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

If y = f(g(x)), then:
    dy/dx = (dy/dg) Ã— (dg/dx) = f'(g(x)) Ã— g'(x)

WHY THIS MATTERS IN NEURAL NETWORKS:
Neural networks are compositions of functions!

Layer 1: hâ‚ = fâ‚(Wâ‚ @ x + bâ‚)
Layer 2: hâ‚‚ = fâ‚‚(Wâ‚‚ @ hâ‚ + bâ‚‚)
Output:  y  = fâ‚ƒ(Wâ‚ƒ @ hâ‚‚ + bâ‚ƒ)

To find âˆ‚Loss/âˆ‚Wâ‚, we use the chain rule repeatedly.
This is called BACKPROPAGATION!
"""

def example_chain_rule():
    """Demonstrate the chain rule with a neural network example."""
    
    print("THE CHAIN RULE (Backpropagation)")
    print("=" * 60)
    
    def relu(x):
        return np.maximum(0, x)
    
    def relu_derivative(x):
        return (x > 0).astype(float)
    
    # Simple 2-layer network
    x = 2.0
    target = 5.0
    w1, b1 = 0.5, 0.1
    w2, b2 = 0.8, 0.2
    
    # Forward pass
    z1 = w1 * x + b1
    h = relu(z1)
    y = w2 * h + b2
    loss = (y - target) ** 2
    
    print(f"Forward pass:")
    print(f"  x={x}, target={target}")
    print(f"  z1={z1}, h={h}, y={y}")
    print(f"  Loss = {loss:.4f}")
    
    # Backward pass (chain rule)
    dL_dy = 2 * (y - target)
    dy_dh = w2
    dh_dz1 = relu_derivative(z1)
    dz1_dw1 = x
    
    dL_dw1 = dL_dy * dy_dh * dh_dz1 * dz1_dw1
    
    print(f"\nBackward pass:")
    print(f"  âˆ‚L/âˆ‚w1 = {dL_dw1:.4f}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GRADIENT DESCENT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
GRADIENT DESCENT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

THE ALGORITHM:
    Initialize Î¸ randomly
    
    Repeat until convergence:
        1. Compute gradient: âˆ‡L(Î¸)
        2. Update: Î¸ = Î¸ - Î± Ã— âˆ‡L(Î¸)
    
    Where Î± (alpha) is the learning rate.

LEARNING RATE:
â€¢ Too small: Convergence is slow
â€¢ Too large: May overshoot and diverge
â€¢ Just right: Smooth convergence

VARIANTS:
â€¢ Batch GD: Gradient over ALL examples (stable but slow)
â€¢ SGD: Gradient on ONE example (fast but noisy)
â€¢ Mini-batch: Gradient on small batch (best of both)
"""

def example_gradient_descent():
    """Complete gradient descent implementation."""
    
    print("GRADIENT DESCENT")
    print("=" * 60)
    
    # 1D example: minimize f(x) = (x-3)Â² + 2
    def f(x):
        return (x - 3)**2 + 2
    
    def df(x):
        return 2 * (x - 3)
    
    x = 0.0
    learning_rate = 0.1
    
    print(f"Minimizing f(x) = (x-3)Â² + 2")
    print(f"Minimum at x = 3")
    print(f"\nIteration | x        | f(x)")
    print("-" * 35)
    
    for i in range(15):
        print(f"{i:9d} | {x:8.4f} | {f(x):8.4f}")
        x = x - learning_rate * df(x)
    
    print(f"\nConverged to x = {x:.4f}")


def example_gradient_descent_linear_regression():
    """Gradient descent for linear regression."""
    
    print("\nGRADIENT DESCENT FOR LINEAR REGRESSION")
    print("=" * 60)
    
    # Generate data
    np.random.seed(42)
    n = 100
    X = np.random.randn(n, 1)
    y = 3 * X.squeeze() + 2 + np.random.randn(n) * 0.5
    
    # Add bias term
    X_b = np.c_[np.ones((n, 1)), X]
    
    def compute_loss(X, y, theta):
        predictions = X @ theta
        return np.mean((predictions - y) ** 2)
    
    def compute_gradient(X, y, theta):
        predictions = X @ theta
        return (2 / len(y)) * X.T @ (predictions - y)
    
    # Mini-batch gradient descent
    theta = np.zeros(2)
    lr = 0.1
    batch_size = 32
    epochs = 100
    
    for epoch in range(epochs):
        indices = np.random.permutation(n)
        for start in range(0, n, batch_size):
            batch_idx = indices[start:start+batch_size]
            gradient = compute_gradient(X_b[batch_idx], y[batch_idx], theta)
            theta = theta - lr * gradient
    
    print(f"True parameters: b=2.0, w=3.0")
    print(f"Learned: b={theta[0]:.4f}, w={theta[1]:.4f}")
    print(f"Final loss: {compute_loss(X_b, y, theta):.6f}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2.3 PROBABILITY AND STATISTICS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
PROBABILITY AND STATISTICS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ML is fundamentally about learning from uncertain data.

PROBABILITY NOTATION:
â€¢ P(A)    = Probability of event A
â€¢ P(A|B)  = Probability of A given B
â€¢ P(A,B)  = Joint probability of A and B

FUNDAMENTAL RULES:
1. Sum rule: P(AâˆªB) = P(A) + P(B) - P(Aâˆ©B)
2. Product rule: P(A,B) = P(A|B) Ã— P(B)
3. Marginalization: P(A) = Î£_b P(A,B=b)

COMMON DISTRIBUTIONS:
â€¢ Bernoulli: Single binary trial
â€¢ Binomial: Number of successes in n trials
â€¢ Normal: The bell curve (everywhere in ML!)
â€¢ Poisson: Count of events in fixed interval
"""

def example_probability_basics():
    """Demonstrate probability concepts."""
    
    print("PROBABILITY BASICS")
    print("=" * 60)
    
    np.random.seed(42)
    
    # Bernoulli and Binomial
    print("\n1. BERNOULLI AND BINOMIAL")
    p = 0.7
    n_trials = 10
    samples = np.random.binomial(n=n_trials, p=p, size=10000)
    
    print(f"Binomial(n={n_trials}, p={p}):")
    print(f"  Theoretical mean = {n_trials * p}")
    print(f"  Empirical mean = {samples.mean():.2f}")
    
    # Normal distribution
    print("\n2. NORMAL DISTRIBUTION")
    mu, sigma = 100, 15
    samples = np.random.normal(mu, sigma, 10000)
    
    print(f"Normal(Î¼={mu}, Ïƒ={sigma}):")
    print(f"  Mean: {samples.mean():.2f}")
    print(f"  Std: {samples.std():.2f}")
    
    # 68-95-99.7 rule
    within_1std = ((mu-sigma <= samples) & (samples <= mu+sigma)).mean()
    within_2std = ((mu-2*sigma <= samples) & (samples <= mu+2*sigma)).mean()
    print(f"  Within 1Ïƒ: {within_1std:.1%} (theory: 68.3%)")
    print(f"  Within 2Ïƒ: {within_2std:.1%} (theory: 95.4%)")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# BAYES' THEOREM
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
BAYES' THEOREM
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

              P(B|A) Ã— P(A)
    P(A|B) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                  P(B)

TERMINOLOGY:
â€¢ P(A)    = Prior: Belief before seeing evidence
â€¢ P(B|A)  = Likelihood: Probability of evidence if A is true
â€¢ P(A|B)  = Posterior: Updated belief after seeing evidence
â€¢ P(B)    = Evidence: Overall probability of observation
"""

def example_bayes_theorem():
    """Demonstrate Bayes' theorem."""
    
    print("BAYES' THEOREM")
    print("=" * 60)
    
    # Medical diagnosis example
    p_disease = 0.01  # 1% have disease
    p_positive_given_disease = 0.99  # 99% sensitivity
    p_positive_given_no_disease = 0.05  # 5% false positive
    
    # P(+) using marginalization
    p_positive = (p_positive_given_disease * p_disease + 
                  p_positive_given_no_disease * (1 - p_disease))
    
    # Bayes' theorem
    p_disease_given_positive = (p_positive_given_disease * p_disease) / p_positive
    
    print("Medical Diagnosis Example:")
    print(f"  P(Disease) = {p_disease:.1%}")
    print(f"  P(+|Disease) = {p_positive_given_disease:.1%}")
    print(f"  P(+|No Disease) = {p_positive_given_no_disease:.1%}")
    print(f"\nResult:")
    print(f"  P(Disease|+) = {p_disease_given_positive:.1%}")
    print(f"\n  Despite 99% test accuracy, positive test only means")
    print(f"  {p_disease_given_positive:.1%} chance of disease!")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MAXIMUM LIKELIHOOD ESTIMATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
MAXIMUM LIKELIHOOD ESTIMATION (MLE)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Find parameters Î¸ that maximize the probability of observed data:
    Î¸_MLE = argmax P(Data | Î¸)

CONNECTION TO ML:
â€¢ MSE Loss = MLE with Gaussian noise assumption
â€¢ Cross-Entropy = MLE with Bernoulli/Categorical distribution
"""

def example_mle():
    """Demonstrate Maximum Likelihood Estimation."""
    
    print("MAXIMUM LIKELIHOOD ESTIMATION")
    print("=" * 60)
    
    np.random.seed(42)
    
    # MLE for coin flip
    true_p = 0.7
    flips = np.random.binomial(1, true_p, 100)
    p_mle = flips.mean()
    
    print("MLE for Coin Flip:")
    print(f"  True p = {true_p}")
    print(f"  MLE estimate = {p_mle}")
    
    # MLE for Gaussian
    true_mu, true_sigma = 5.0, 2.0
    data = np.random.normal(true_mu, true_sigma, 200)
    
    print("\nMLE for Gaussian:")
    print(f"  True: Î¼={true_mu}, Ïƒ={true_sigma}")
    print(f"  MLE:  Î¼={data.mean():.4f}, Ïƒ={data.std():.4f}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DESCRIPTIVE STATISTICS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def example_descriptive_statistics():
    """Demonstrate descriptive statistics."""
    
    print("DESCRIPTIVE STATISTICS")
    print("=" * 60)
    
    np.random.seed(42)
    
    # Normal data
    data = np.random.normal(50, 10, 1000)
    
    print("Normal Distribution Data:")
    print(f"  Mean: {np.mean(data):.2f}")
    print(f"  Median: {np.median(data):.2f}")
    print(f"  Std: {np.std(data):.2f}")
    print(f"  IQR: {np.percentile(data, 75) - np.percentile(data, 25):.2f}")
    print(f"  Skewness: {stats.skew(data):.3f}")
    print(f"  Kurtosis: {stats.kurtosis(data):.3f}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2.4 INFORMATION THEORY
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
INFORMATION THEORY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ENTROPY: Measures uncertainty in a random variable
    H(X) = -Î£áµ¢ P(xáµ¢) Ã— logâ‚‚ P(xáµ¢)

â€¢ Low entropy: Predictable (one outcome likely)
â€¢ High entropy: Unpredictable (all outcomes equally likely)

CROSS-ENTROPY: Measures difference between distributions
    H(P, Q) = -Î£áµ¢ P(xáµ¢) Ã— log Q(xáµ¢)

This is THE loss function for classification!

KL DIVERGENCE: Measures how Q differs from P
    D_KL(P || Q) = H(P, Q) - H(P)
"""

def example_entropy():
    """Demonstrate entropy."""
    
    print("ENTROPY")
    print("=" * 60)
    
    def entropy(probs):
        probs = np.array(probs)
        probs = probs[probs > 0]
        return -np.sum(probs * np.log2(probs))
    
    print("Binary Entropy (Coin Flip):")
    print("P(heads) | Entropy")
    print("-" * 25)
    
    for p in [0.1, 0.3, 0.5, 0.7, 0.9]:
        h = entropy([p, 1-p])
        bar = "â–ˆ" * int(h * 20)
        print(f"  {p:.1f}    | {h:.3f}  {bar}")
    
    print("\nMaximum entropy at p=0.5 (most uncertain)")


def example_cross_entropy():
    """Demonstrate cross-entropy loss."""
    
    print("\nCROSS-ENTROPY")
    print("=" * 60)
    
    def binary_cross_entropy(y_true, y_pred):
        epsilon = 1e-15
        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
        return -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
    
    print("Binary Cross-Entropy (True label = 1):")
    print("Predicted | BCE Loss")
    print("-" * 25)
    
    for pred in [0.1, 0.3, 0.5, 0.7, 0.9, 0.99]:
        loss = binary_cross_entropy(1, pred)
        bar = "â–ˆ" * int(loss * 5)
        print(f"  {pred:.2f}    | {loss:.3f}  {bar}")


def example_kl_divergence():
    """Demonstrate KL divergence."""
    
    print("\nKL DIVERGENCE")
    print("=" * 60)
    
    def kl_divergence(p, q):
        p, q = np.array(p), np.array(q)
        p, q = np.clip(p, 1e-10, 1), np.clip(q, 1e-10, 1)
        return np.sum(p * np.log(p / q))
    
    P = [0.4, 0.3, 0.2, 0.1]
    Q1 = [0.25, 0.25, 0.25, 0.25]
    Q2 = [0.1, 0.1, 0.4, 0.4]
    
    print(f"P = {P}")
    print(f"Q1 (uniform) = {Q1}")
    print(f"Q2 (different) = {Q2}")
    print(f"\nD_KL(P || Q1) = {kl_divergence(P, Q1):.4f}")
    print(f"D_KL(P || Q2) = {kl_divergence(P, Q2):.4f}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CHAPTER 2 SUMMARY
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
CHAPTER 2 SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

LINEAR ALGEBRA:
â€¢ Vectors and matrices are the data structures of ML
â€¢ Matrix multiplication is the core operation
â€¢ Eigenvalues/SVD enable PCA and dimensionality reduction

CALCULUS:
â€¢ Gradients tell us which direction to move parameters
â€¢ Chain rule enables backpropagation
â€¢ Gradient descent iteratively minimizes loss

PROBABILITY:
â€¢ Bayes' theorem updates beliefs given evidence
â€¢ MLE finds parameters that maximize data likelihood
â€¢ Many loss functions = negative log-likelihood

INFORMATION THEORY:
â€¢ Entropy measures uncertainty
â€¢ Cross-entropy is THE classification loss
â€¢ KL divergence measures distribution difference


KEY FORMULAS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Gradient Descent:    Î¸ = Î¸ - Î± Ã— âˆ‡L(Î¸)
Bayes' Theorem:      P(A|B) = P(B|A)P(A) / P(B)
Entropy:             H(X) = -Î£ P(x) log P(x)
Cross-Entropy:       H(P,Q) = -Î£ P(x) log Q(x)
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Run all examples
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    print("\n" + "="*70)
    print("CHAPTER 2: MATHEMATICS FOR MACHINE LEARNING")
    print("="*70)
    
    example_scalars_vectors_matrices()
    print("\n")
    example_matrix_operations()
    print("\n")
    example_eigenvalues()
    print("\n")
    example_svd()
    print("\n")
    example_derivatives_gradients()
    print("\n")
    example_chain_rule()
    print("\n")
    example_gradient_descent()
    example_gradient_descent_linear_regression()
    print("\n")
    example_probability_basics()
    print("\n")
    example_bayes_theorem()
    print("\n")
    example_mle()
    print("\n")
    example_descriptive_statistics()
    print("\n")
    example_entropy()
    example_cross_entropy()
    example_kl_divergence()
# â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
#                                                                               
#   CHAPTER 3: DATA FUNDAMENTALS                                                 
#                                                                               
# â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ

"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘   "Data is the new oil. But like oil, it's valuable only when refined."       â•‘
â•‘                                              â€” Clive Humby                    â•‘
â•‘                                                                               â•‘
â•‘   Data preparation often takes 60-80% of a data scientist's time.             â•‘
â•‘   Master this chapter, and you'll be ahead of most practitioners.             â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, KFold, StratifiedKFold
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder
from sklearn.impute import SimpleImputer
import warnings
warnings.filterwarnings('ignore')


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3.1 UNDERSTANDING YOUR DATA
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
UNDERSTANDING YOUR DATA
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Before building models, you MUST understand your data thoroughly.
"Garbage in, garbage out" - no algorithm can fix bad data.


DATA TYPES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. NUMERICAL (Quantitative)
   a) Continuous: Can take any value (height, temperature, price)
   b) Discrete: Countable values (number of children, items sold)

2. CATEGORICAL (Qualitative)
   a) Nominal: No inherent order (color, country, gender)
   b) Ordinal: Has order but no meaningful distance (rating: low/medium/high)

3. TEXT: Unstructured text data (reviews, documents)

4. DATE/TIME: Temporal data (timestamps, dates)

5. BINARY: Two values (True/False, 0/1, Yes/No)


COMMON DATA STRUCTURES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

TABULAR (Most common in traditional ML):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Sample   â”‚ Feature1â”‚Feature2â”‚ Feature3 â”‚  Label  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Row 1    â”‚   ...   â”‚   ...  â”‚   ...    â”‚   ...   â”‚
â”‚ Row 2    â”‚   ...   â”‚   ...  â”‚   ...    â”‚   ...   â”‚
â”‚ ...      â”‚   ...   â”‚   ...  â”‚   ...    â”‚   ...   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

IMAGES: 3D tensors (height Ã— width Ã— channels)
TEXT: Sequences of tokens
TIME SERIES: Sequences with temporal ordering
GRAPHS: Nodes and edges
"""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3.1.1 EXPLORATORY DATA ANALYSIS (EDA)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
EXPLORATORY DATA ANALYSIS (EDA)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EDA is the process of investigating data to discover patterns, spot anomalies,
and check assumptions using statistical graphics and summary statistics.

THE EDA CHECKLIST:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â–¡ Dataset shape and size
â–¡ Data types of each column
â–¡ Missing values
â–¡ Summary statistics (mean, median, std, min, max)
â–¡ Distribution of each feature
â–¡ Relationships between features (correlations)
â–¡ Class distribution (for classification)
â–¡ Outliers
â–¡ Duplicate rows
"""

def create_sample_dataset():
    """Create a realistic sample dataset for demonstration."""
    np.random.seed(42)
    n = 1000
    
    # Generate features
    data = {
        'age': np.random.randint(18, 80, n),
        'income': np.random.exponential(50000, n) + 20000,
        'credit_score': np.random.normal(700, 50, n).clip(300, 850),
        'years_employed': np.random.exponential(5, n).clip(0, 40),
        'num_credit_cards': np.random.poisson(3, n),
        'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], 
                                       n, p=[0.4, 0.35, 0.2, 0.05]),
        'home_ownership': np.random.choice(['Own', 'Rent', 'Mortgage'], 
                                           n, p=[0.3, 0.4, 0.3]),
        'loan_amount': np.random.uniform(1000, 50000, n),
    }
    
    # Generate target based on features (loan default: 0 or 1)
    default_prob = (
        (data['income'] < 40000).astype(float) * 0.2 +
        (data['credit_score'] < 650).astype(float) * 0.3 +
        (data['years_employed'] < 1).astype(float) * 0.2 +
        np.random.random(n) * 0.3
    )
    data['defaulted'] = (np.random.random(n) < default_prob / default_prob.max() * 0.3).astype(int)
    
    df = pd.DataFrame(data)
    
    # Introduce some missing values (realistic scenario)
    missing_mask = np.random.random(n) < 0.05
    df.loc[missing_mask, 'income'] = np.nan
    
    missing_mask = np.random.random(n) < 0.03
    df.loc[missing_mask, 'years_employed'] = np.nan
    
    return df


def example_eda_comprehensive():
    """Comprehensive EDA demonstration."""
    
    print("EXPLORATORY DATA ANALYSIS (EDA)")
    print("=" * 70)
    
    df = create_sample_dataset()
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # STEP 1: Basic Information
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("STEP 1: BASIC INFORMATION")
    print("â”€" * 70)
    
    print(f"\nDataset Shape: {df.shape}")
    print(f"  - Rows (samples): {df.shape[0]}")
    print(f"  - Columns (features): {df.shape[1]}")
    
    print(f"\nMemory Usage: {df.memory_usage(deep=True).sum() / 1024:.2f} KB")
    
    print("\nColumn Data Types:")
    for col, dtype in df.dtypes.items():
        print(f"  {col:<20}: {dtype}")
    
    print("\nFirst 5 Rows:")
    print(df.head().to_string())
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # STEP 2: Missing Values Analysis
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("STEP 2: MISSING VALUES ANALYSIS")
    print("â”€" * 70)
    
    missing = df.isnull().sum()
    missing_pct = (missing / len(df)) * 100
    
    print("\nMissing Values Summary:")
    print(f"{'Column':<20} {'Missing':<10} {'Percentage':<10}")
    print("-" * 40)
    
    for col in df.columns:
        if missing[col] > 0:
            print(f"{col:<20} {missing[col]:<10} {missing_pct[col]:.2f}%")
    
    total_missing = df.isnull().sum().sum()
    total_cells = df.size
    print(f"\nTotal missing: {total_missing} / {total_cells} ({total_missing/total_cells*100:.2f}%)")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # STEP 3: Numerical Features Summary
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("STEP 3: NUMERICAL FEATURES SUMMARY")
    print("â”€" * 70)
    
    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    print(f"\nNumerical columns: {numerical_cols}")
    
    print("\nDescriptive Statistics:")
    print(df[numerical_cols].describe().round(2).to_string())
    
    # Additional statistics
    print("\nAdditional Statistics:")
    print(f"{'Column':<20} {'Skewness':<12} {'Kurtosis':<12}")
    print("-" * 44)
    for col in numerical_cols:
        skew = df[col].skew()
        kurt = df[col].kurtosis()
        print(f"{col:<20} {skew:>10.2f} {kurt:>10.2f}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # STEP 4: Categorical Features Summary
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("STEP 4: CATEGORICAL FEATURES SUMMARY")
    print("â”€" * 70)
    
    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
    print(f"\nCategorical columns: {categorical_cols}")
    
    for col in categorical_cols:
        print(f"\n{col}:")
        value_counts = df[col].value_counts()
        for val, count in value_counts.items():
            pct = count / len(df) * 100
            bar = "â–ˆ" * int(pct / 2)
            print(f"  {val:<15} {count:>5} ({pct:>5.1f}%) {bar}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # STEP 5: Target Variable Analysis
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("STEP 5: TARGET VARIABLE ANALYSIS")
    print("â”€" * 70)
    
    target = 'defaulted'
    print(f"\nTarget column: {target}")
    print("\nClass Distribution:")
    
    class_counts = df[target].value_counts()
    for val, count in class_counts.items():
        pct = count / len(df) * 100
        bar = "â–ˆ" * int(pct / 2)
        label = "No Default" if val == 0 else "Default"
        print(f"  {label:<15} {count:>5} ({pct:>5.1f}%) {bar}")
    
    # Check for class imbalance
    imbalance_ratio = class_counts.max() / class_counts.min()
    print(f"\nClass Imbalance Ratio: {imbalance_ratio:.2f}")
    if imbalance_ratio > 3:
        print("  âš ï¸  Significant class imbalance detected!")
        print("  Consider: oversampling, undersampling, or class weights")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # STEP 6: Correlation Analysis
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("STEP 6: CORRELATION ANALYSIS")
    print("â”€" * 70)
    
    # Correlation matrix
    corr_matrix = df[numerical_cols].corr()
    
    print("\nCorrelation Matrix:")
    print(corr_matrix.round(2).to_string())
    
    # Find highly correlated pairs
    print("\nHighly Correlated Pairs (|r| > 0.5):")
    high_corr_pairs = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            if abs(corr_matrix.iloc[i, j]) > 0.5:
                high_corr_pairs.append((
                    corr_matrix.columns[i],
                    corr_matrix.columns[j],
                    corr_matrix.iloc[i, j]
                ))
    
    if high_corr_pairs:
        for col1, col2, corr in high_corr_pairs:
            print(f"  {col1} <-> {col2}: {corr:.3f}")
    else:
        print("  No highly correlated pairs found")
    
    # Correlation with target
    print("\nCorrelation with Target:")
    target_corr = df[numerical_cols].corrwith(df[target]).sort_values(key=abs, ascending=False)
    for col, corr in target_corr.items():
        if col != target:
            print(f"  {col:<20}: {corr:>7.3f}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # STEP 7: Outlier Detection
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("STEP 7: OUTLIER DETECTION")
    print("â”€" * 70)
    
    print("\nOutliers using IQR method:")
    
    for col in numerical_cols:
        if col == target:
            continue
        
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
        
        if len(outliers) > 0:
            print(f"\n  {col}:")
            print(f"    Bounds: [{lower_bound:.2f}, {upper_bound:.2f}]")
            print(f"    Outliers: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # STEP 8: Duplicate Analysis
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("STEP 8: DUPLICATE ANALYSIS")
    print("â”€" * 70)
    
    duplicates = df.duplicated().sum()
    print(f"\nDuplicate rows: {duplicates} ({duplicates/len(df)*100:.2f}%)")
    
    return df


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3.2 DATA CLEANING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
DATA CLEANING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Real-world data is messy. Data cleaning transforms raw data into a format
suitable for analysis and modeling.

COMMON DATA QUALITY ISSUES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Missing values
â€¢ Duplicate records
â€¢ Inconsistent formatting
â€¢ Invalid values (negative age, impossible dates)
â€¢ Outliers
â€¢ Data entry errors
"""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3.2.1 HANDLING MISSING VALUES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
HANDLING MISSING VALUES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Missing values can be:
â€¢ MCAR (Missing Completely At Random): Missingness is random
â€¢ MAR (Missing At Random): Depends on observed data
â€¢ MNAR (Missing Not At Random): Depends on unobserved data

STRATEGIES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. DELETION
   a) Listwise deletion: Remove rows with any missing value
   b) Pairwise deletion: Remove rows only for specific analyses
   
   Pros: Simple, preserves distribution
   Cons: Loses data, can introduce bias

2. IMPUTATION
   a) Simple imputation:
      - Mean/Median (numerical)
      - Mode (categorical)
      - Constant value
   
   b) Statistical imputation:
      - Regression imputation
      - KNN imputation
   
   c) Model-based:
      - MICE (Multiple Imputation by Chained Equations)
      - IterativeImputer

3. INDICATOR VARIABLE
   Create a binary column indicating missingness
   (Can capture "missingness as information")
"""

def example_handling_missing_values():
    """Demonstrate different methods for handling missing values."""
    
    print("HANDLING MISSING VALUES")
    print("=" * 70)
    
    # Create sample data with missing values
    np.random.seed(42)
    n = 100
    
    df = pd.DataFrame({
        'age': np.random.randint(20, 70, n).astype(float),
        'income': np.random.normal(50000, 15000, n),
        'credit_score': np.random.normal(700, 50, n),
        'category': np.random.choice(['A', 'B', 'C'], n)
    })
    
    # Introduce missing values
    df.loc[np.random.choice(n, 10, replace=False), 'age'] = np.nan
    df.loc[np.random.choice(n, 15, replace=False), 'income'] = np.nan
    df.loc[np.random.choice(n, 8, replace=False), 'category'] = np.nan
    
    print("\nOriginal Data with Missing Values:")
    print(f"Shape: {df.shape}")
    print("\nMissing values per column:")
    print(df.isnull().sum())
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # METHOD 1: Deletion
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("METHOD 1: DELETION")
    print("â”€" * 70)
    
    # Listwise deletion (drop any row with missing values)
    df_dropped = df.dropna()
    print(f"\nAfter dropping rows with any NaN:")
    print(f"  Original rows: {len(df)}")
    print(f"  Remaining rows: {len(df_dropped)}")
    print(f"  Lost: {len(df) - len(df_dropped)} rows ({(len(df) - len(df_dropped))/len(df)*100:.1f}%)")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # METHOD 2: Simple Imputation
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("METHOD 2: SIMPLE IMPUTATION")
    print("â”€" * 70)
    
    df_imputed = df.copy()
    
    # Mean imputation for numerical columns
    print("\nNumerical columns - Mean imputation:")
    for col in ['age', 'income']:
        mean_val = df_imputed[col].mean()
        df_imputed[col].fillna(mean_val, inplace=True)
        print(f"  {col}: filled with mean = {mean_val:.2f}")
    
    # Mode imputation for categorical columns
    print("\nCategorical columns - Mode imputation:")
    mode_val = df_imputed['category'].mode()[0]
    df_imputed['category'].fillna(mode_val, inplace=True)
    print(f"  category: filled with mode = '{mode_val}'")
    
    print(f"\nMissing values after imputation: {df_imputed.isnull().sum().sum()}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # METHOD 3: Using sklearn SimpleImputer
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("METHOD 3: SKLEARN SIMPLEIMPUTER")
    print("â”€" * 70)
    
    from sklearn.impute import SimpleImputer
    
    df_sklearn = df.copy()
    numerical_cols = ['age', 'income', 'credit_score']
    
    # Median imputation (more robust to outliers)
    imputer = SimpleImputer(strategy='median')
    df_sklearn[numerical_cols] = imputer.fit_transform(df_sklearn[numerical_cols])
    
    print("\nUsing SimpleImputer with strategy='median':")
    print(f"  Imputed values: {imputer.statistics_}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # METHOD 4: KNN Imputation
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("METHOD 4: KNN IMPUTATION")
    print("â”€" * 70)
    
    from sklearn.impute import KNNImputer
    
    df_knn = df.copy()
    
    # KNN imputation (uses similar samples to impute)
    knn_imputer = KNNImputer(n_neighbors=5)
    df_knn[numerical_cols] = knn_imputer.fit_transform(df_knn[numerical_cols])
    
    print("\nKNN Imputation (k=5):")
    print("  Uses values from 5 nearest neighbors to impute missing values")
    print("  Better preserves relationships between features")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # METHOD 5: Add Missing Indicator
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("METHOD 5: MISSING INDICATOR")
    print("â”€" * 70)
    
    df_indicator = df.copy()
    
    # Create indicator columns
    for col in ['age', 'income']:
        indicator_col = f'{col}_was_missing'
        df_indicator[indicator_col] = df_indicator[col].isnull().astype(int)
    
    # Then impute
    df_indicator['age'].fillna(df_indicator['age'].median(), inplace=True)
    df_indicator['income'].fillna(df_indicator['income'].median(), inplace=True)
    
    print("\nAdded indicator columns:")
    print(df_indicator[['age', 'age_was_missing', 'income', 'income_was_missing']].head(10).to_string())
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # SUMMARY
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("SUMMARY: CHOOSING A METHOD")
    print("â”€" * 70)
    print("""
    METHOD              WHEN TO USE
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Deletion            Few missing values (<5%), MCAR data
    Mean/Median         Simple baseline, numerical data
    Mode                Categorical data
    KNN Imputation      When feature relationships matter
    MICE/Iterative      Complex missing patterns, high-quality imputation
    Missing Indicator   When missingness itself is informative
    
    GENERAL ADVICE:
    â€¢ Always analyze WHY data is missing before choosing a method
    â€¢ Test multiple methods and compare model performance
    â€¢ For tree-based models, consider using native missing value handling
    """)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3.2.2 DEALING WITH OUTLIERS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
DEALING WITH OUTLIERS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Outliers are data points that differ significantly from other observations.

DETECTION METHODS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. STATISTICAL METHODS
   a) Z-score: Points with |z| > 3 are outliers
   b) IQR method: Points outside [Q1 - 1.5*IQR, Q3 + 1.5*IQR]
   c) Modified Z-score: Uses median instead of mean (more robust)

2. MACHINE LEARNING METHODS
   a) Isolation Forest
   b) Local Outlier Factor (LOF)
   c) One-Class SVM

HANDLING STRATEGIES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Remove outliers (if they're errors)
â€¢ Cap/Floor (Winsorization)
â€¢ Transform data (log, Box-Cox)
â€¢ Keep outliers (if they're real)
â€¢ Use robust models
"""

def example_outlier_detection():
    """Demonstrate outlier detection methods."""
    
    print("OUTLIER DETECTION AND HANDLING")
    print("=" * 70)
    
    # Create data with outliers
    np.random.seed(42)
    n = 200
    
    # Normal data
    normal_data = np.random.normal(50, 10, n - 10)
    # Add some outliers
    outliers = np.array([10, 15, 95, 100, 105, 110, 5, 120, 0, 130])
    data = np.concatenate([normal_data, outliers])
    
    df = pd.DataFrame({'value': data})
    
    print("\nData Summary:")
    print(df['value'].describe().round(2))
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # METHOD 1: Z-Score
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("METHOD 1: Z-SCORE")
    print("â”€" * 70)
    
    from scipy import stats
    
    z_scores = np.abs(stats.zscore(df['value']))
    z_threshold = 3
    
    z_outliers = df[z_scores > z_threshold]
    
    print(f"\nZ-score threshold: {z_threshold}")
    print(f"Outliers detected: {len(z_outliers)}")
    print(f"Outlier values: {z_outliers['value'].values.round(2)}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # METHOD 2: IQR Method
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("METHOD 2: IQR METHOD (Box Plot)")
    print("â”€" * 70)
    
    Q1 = df['value'].quantile(0.25)
    Q3 = df['value'].quantile(0.75)
    IQR = Q3 - Q1
    
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    iqr_outliers = df[(df['value'] < lower_bound) | (df['value'] > upper_bound)]
    
    print(f"\nQ1 = {Q1:.2f}, Q3 = {Q3:.2f}, IQR = {IQR:.2f}")
    print(f"Lower bound: {lower_bound:.2f}")
    print(f"Upper bound: {upper_bound:.2f}")
    print(f"Outliers detected: {len(iqr_outliers)}")
    print(f"Outlier values: {sorted(iqr_outliers['value'].values.round(2))}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # METHOD 3: Isolation Forest
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("METHOD 3: ISOLATION FOREST")
    print("â”€" * 70)
    
    from sklearn.ensemble import IsolationForest
    
    iso_forest = IsolationForest(contamination=0.05, random_state=42)
    outlier_labels = iso_forest.fit_predict(df[['value']])
    
    iso_outliers = df[outlier_labels == -1]
    
    print(f"\nContamination parameter: 5%")
    print(f"Outliers detected: {len(iso_outliers)}")
    print(f"Outlier values: {sorted(iso_outliers['value'].values.round(2))}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # HANDLING STRATEGIES
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("HANDLING STRATEGIES")
    print("â”€" * 70)
    
    # Strategy 1: Remove
    df_removed = df[(df['value'] >= lower_bound) & (df['value'] <= upper_bound)]
    print(f"\n1. Remove outliers:")
    print(f"   Before: {len(df)} rows")
    print(f"   After: {len(df_removed)} rows")
    
    # Strategy 2: Cap (Winsorization)
    df_capped = df.copy()
    df_capped['value'] = df_capped['value'].clip(lower_bound, upper_bound)
    print(f"\n2. Cap/Floor (Winsorization):")
    print(f"   Values clipped to [{lower_bound:.2f}, {upper_bound:.2f}]")
    print(f"   New max: {df_capped['value'].max():.2f}")
    print(f"   New min: {df_capped['value'].min():.2f}")
    
    # Strategy 3: Transform
    df_log = df.copy()
    df_log['value_log'] = np.log1p(df_log['value'] - df_log['value'].min() + 1)
    print(f"\n3. Log transform:")
    print(f"   Original skewness: {df['value'].skew():.3f}")
    print(f"   After log: {df_log['value_log'].skew():.3f}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3.3 FEATURE ENGINEERING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
FEATURE ENGINEERING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Feature engineering is the process of creating new features from existing ones
to improve model performance. Often the most impactful part of ML work.

"Coming up with features is difficult, time-consuming, requires expert 
knowledge. 'Applied machine learning' is basically feature engineering."
                                        â€” Andrew Ng
"""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3.3.1 FEATURE CREATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def example_feature_creation():
    """Demonstrate various feature creation techniques."""
    
    print("FEATURE CREATION")
    print("=" * 70)
    
    # Sample data
    np.random.seed(42)
    n = 1000
    
    df = pd.DataFrame({
        'date': pd.date_range('2023-01-01', periods=n, freq='H'),
        'price': np.random.uniform(10, 100, n),
        'quantity': np.random.randint(1, 50, n),
        'customer_age': np.random.randint(18, 70, n),
        'category': np.random.choice(['Electronics', 'Clothing', 'Food'], n),
        'city': np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston'], n),
        'review_text': np.random.choice([
            'Great product!', 'Not bad', 'Terrible', 'Love it', 'Okay'
        ], n)
    })
    
    print("\nOriginal Features:")
    print(df.head().to_string())
    print(f"\nShape: {df.shape}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 1. MATHEMATICAL TRANSFORMATIONS
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("1. MATHEMATICAL TRANSFORMATIONS")
    print("â”€" * 70)
    
    # Arithmetic combinations
    df['total_value'] = df['price'] * df['quantity']
    df['price_per_unit_log'] = np.log1p(df['price'])
    df['quantity_squared'] = df['quantity'] ** 2
    
    print("\nCreated:")
    print("  â€¢ total_value = price Ã— quantity")
    print("  â€¢ price_per_unit_log = log(1 + price)")
    print("  â€¢ quantity_squared = quantityÂ²")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 2. DATE/TIME FEATURES
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("2. DATE/TIME FEATURES")
    print("â”€" * 70)
    
    df['hour'] = df['date'].dt.hour
    df['day_of_week'] = df['date'].dt.dayofweek  # 0=Monday
    df['day_of_month'] = df['date'].dt.day
    df['month'] = df['date'].dt.month
    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
    df['is_morning'] = ((df['hour'] >= 6) & (df['hour'] < 12)).astype(int)
    df['is_evening'] = ((df['hour'] >= 18) & (df['hour'] < 22)).astype(int)
    
    # Cyclical encoding for periodic features
    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
    
    print("\nDate/Time features created:")
    print("  â€¢ hour, day_of_week, day_of_month, month")
    print("  â€¢ is_weekend, is_morning, is_evening")
    print("  â€¢ hour_sin, hour_cos (cyclical encoding)")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 3. BINNING/DISCRETIZATION
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("3. BINNING/DISCRETIZATION")
    print("â”€" * 70)
    
    # Age groups
    df['age_group'] = pd.cut(
        df['customer_age'],
        bins=[0, 25, 35, 50, 100],
        labels=['Young', 'Adult', 'Middle-aged', 'Senior']
    )
    
    # Price tiers
    df['price_tier'] = pd.qcut(
        df['price'],
        q=4,
        labels=['Budget', 'Economy', 'Premium', 'Luxury']
    )
    
    print("\nBinning created:")
    print("  â€¢ age_group: Young/Adult/Middle-aged/Senior")
    print("  â€¢ price_tier: Budget/Economy/Premium/Luxury (quartiles)")
    
    print("\nAge group distribution:")
    print(df['age_group'].value_counts().to_string())
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 4. TEXT FEATURES
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("4. TEXT FEATURES")
    print("â”€" * 70)
    
    # Simple text features
    df['review_length'] = df['review_text'].str.len()
    df['review_word_count'] = df['review_text'].str.split().str.len()
    df['has_exclamation'] = df['review_text'].str.contains('!').astype(int)
    
    # Sentiment (simple rule-based)
    positive_words = ['great', 'love', 'excellent', 'good']
    negative_words = ['terrible', 'bad', 'awful', 'hate']
    
    df['has_positive'] = df['review_text'].str.lower().str.contains('|'.join(positive_words)).astype(int)
    df['has_negative'] = df['review_text'].str.lower().str.contains('|'.join(negative_words)).astype(int)
    
    print("\nText features created:")
    print("  â€¢ review_length, review_word_count")
    print("  â€¢ has_exclamation")
    print("  â€¢ has_positive, has_negative (simple sentiment)")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 5. AGGREGATION FEATURES
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("5. AGGREGATION FEATURES")
    print("â”€" * 70)
    
    # Category-level statistics
    category_stats = df.groupby('category').agg({
        'price': ['mean', 'std', 'min', 'max'],
        'quantity': 'mean'
    }).reset_index()
    category_stats.columns = ['category', 'cat_price_mean', 'cat_price_std', 
                               'cat_price_min', 'cat_price_max', 'cat_qty_mean']
    
    df = df.merge(category_stats, on='category', how='left')
    
    # Price relative to category average
    df['price_vs_category'] = df['price'] / df['cat_price_mean']
    
    print("\nAggregation features created:")
    print("  â€¢ Category-level: mean, std, min, max price; mean quantity")
    print("  â€¢ price_vs_category = price / category_mean_price")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # SUMMARY
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("SUMMARY")
    print("â”€" * 70)
    
    print(f"\nOriginal features: 7")
    print(f"Final features: {len(df.columns)}")
    print(f"New features created: {len(df.columns) - 7}")
    
    print("\nFeature engineering techniques used:")
    print("  1. Mathematical transformations (multiply, log, power)")
    print("  2. Date/time extraction (hour, day, weekend, cyclical)")
    print("  3. Binning/Discretization (age groups, price tiers)")
    print("  4. Text features (length, word count, patterns)")
    print("  5. Aggregation features (group statistics)")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3.4 DATA PREPROCESSING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
DATA PREPROCESSING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Preprocessing transforms raw features into a format suitable for ML algorithms.
"""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3.4.1 ENCODING CATEGORICAL VARIABLES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def example_categorical_encoding():
    """Demonstrate categorical encoding methods."""
    
    print("ENCODING CATEGORICAL VARIABLES")
    print("=" * 70)
    
    # Sample data
    df = pd.DataFrame({
        'color': ['Red', 'Blue', 'Green', 'Blue', 'Red', 'Green', 'Blue', 'Red'],
        'size': ['Small', 'Medium', 'Large', 'Small', 'Large', 'Medium', 'Small', 'Large'],
        'brand': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B'],
        'price': [10, 20, 30, 15, 25, 35, 12, 22]
    })
    
    print("\nOriginal Data:")
    print(df.to_string())
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # METHOD 1: Label Encoding
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("METHOD 1: LABEL ENCODING")
    print("â”€" * 70)
    
    """
    Label encoding assigns each category a unique integer.
    
    USE WHEN:
    â€¢ Ordinal variables (has meaningful order)
    â€¢ Tree-based models (can handle arbitrary encoding)
    
    AVOID WHEN:
    â€¢ Nominal variables with linear models (implies false order)
    """
    
    le = LabelEncoder()
    df_label = df.copy()
    df_label['color_encoded'] = le.fit_transform(df['color'])
    
    print("\nLabel Encoding for 'color':")
    print(f"  Mapping: {dict(zip(le.classes_, range(len(le.classes_))))}")
    print(f"\n{df_label[['color', 'color_encoded']].drop_duplicates().to_string()}")
    
    # For ordinal variables with custom order
    size_order = {'Small': 0, 'Medium': 1, 'Large': 2}
    df_label['size_encoded'] = df['size'].map(size_order)
    
    print("\nOrdinal Encoding for 'size' (with custom order):")
    print(f"  Mapping: {size_order}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # METHOD 2: One-Hot Encoding
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("METHOD 2: ONE-HOT ENCODING")
    print("â”€" * 70)
    
    """
    One-hot encoding creates binary columns for each category.
    
    USE WHEN:
    â€¢ Nominal variables (no meaningful order)
    â€¢ Linear models, neural networks
    â€¢ Few unique values
    
    AVOID WHEN:
    â€¢ High cardinality (many unique values)
    â€¢ Tree-based models (less efficient)
    """
    
    # Using pandas
    df_onehot = pd.get_dummies(df, columns=['color'], prefix='color')
    
    print("\nOne-Hot Encoding for 'color':")
    print(df_onehot.to_string())
    
    # Using sklearn
    encoder = OneHotEncoder(sparse_output=False, drop='first')  # drop='first' to avoid multicollinearity
    encoded = encoder.fit_transform(df[['color']])
    feature_names = encoder.get_feature_names_out(['color'])
    
    print(f"\nWith drop='first' (avoiding dummy variable trap):")
    print(f"  Features: {feature_names}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # METHOD 3: Target Encoding (Mean Encoding)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("METHOD 3: TARGET ENCODING (Mean Encoding)")
    print("â”€" * 70)
    
    """
    Target encoding replaces categories with mean of target variable.
    
    USE WHEN:
    â€¢ High cardinality categorical variables
    â€¢ Strong relationship between category and target
    
    CAUTION:
    â€¢ Risk of overfitting (use cross-validation)
    â€¢ Apply smoothing for rare categories
    """
    
    # Simulate target variable
    df['target'] = [1, 0, 1, 1, 0, 1, 0, 0]
    
    # Calculate mean target for each category
    target_means = df.groupby('brand')['target'].mean()
    df['brand_target_encoded'] = df['brand'].map(target_means)
    
    print("\nTarget Encoding for 'brand':")
    print(f"  Brand means: {target_means.to_dict()}")
    print(f"\n{df[['brand', 'target', 'brand_target_encoded']].to_string()}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # METHOD 4: Frequency Encoding
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("METHOD 4: FREQUENCY ENCODING")
    print("â”€" * 70)
    
    """
    Replace categories with their frequency in the dataset.
    
    USE WHEN:
    â€¢ Frequency is meaningful
    â€¢ Want to preserve some information about category
    """
    
    freq_encoding = df['color'].value_counts(normalize=True)
    df['color_freq'] = df['color'].map(freq_encoding)
    
    print("\nFrequency Encoding for 'color':")
    print(f"  Frequencies: {freq_encoding.to_dict()}")
    print(f"\n{df[['color', 'color_freq']].drop_duplicates().to_string()}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # SUMMARY
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("ENCODING METHOD SUMMARY")
    print("â”€" * 70)
    print("""
    METHOD             USE CASE                           PROS/CONS
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Label Encoding     Ordinal data, tree models          Simple, compact
    One-Hot Encoding   Nominal data, linear models        No false order, sparse
    Target Encoding    High cardinality, prediction       Powerful, risk of overfit
    Frequency Encoding Frequency matters                  Simple, no explosion
    Binary Encoding    Medium cardinality                 Compact, some info loss
    """)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3.4.2 FEATURE SCALING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def example_feature_scaling():
    """Demonstrate feature scaling methods."""
    
    print("FEATURE SCALING")
    print("=" * 70)
    
    # Sample data with different scales
    np.random.seed(42)
    df = pd.DataFrame({
        'age': np.random.randint(18, 80, 100),           # Range: 18-80
        'income': np.random.normal(50000, 20000, 100),   # Range: ~10k-90k
        'score': np.random.uniform(0, 1, 100),           # Range: 0-1
        'count': np.random.exponential(10, 100)          # Skewed
    })
    
    print("\nOriginal Data Statistics:")
    print(df.describe().round(2).to_string())
    
    """
    WHY SCALE FEATURES?
    
    1. Gradient descent converges faster when features are on similar scales
    2. Distance-based algorithms (KNN, SVM) treat all features equally
    3. Regularization penalizes features fairly
    
    WHEN TO SCALE:
    â€¢ Linear/Logistic Regression: Yes
    â€¢ SVM, KNN: Yes
    â€¢ Neural Networks: Yes
    â€¢ Tree-based models: Not necessary (but doesn't hurt)
    """
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # METHOD 1: StandardScaler (Z-score normalization)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("METHOD 1: STANDARD SCALER (Z-score)")
    print("â”€" * 70)
    
    """
    StandardScaler: z = (x - mean) / std
    
    Result: mean â‰ˆ 0, std â‰ˆ 1
    
    USE WHEN:
    â€¢ Data is approximately normal
    â€¢ Outliers are few
    â€¢ Most common choice
    """
    
    scaler = StandardScaler()
    df_standard = pd.DataFrame(
        scaler.fit_transform(df),
        columns=df.columns
    )
    
    print("\nAfter StandardScaler:")
    print(df_standard.describe().round(2).to_string())
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # METHOD 2: MinMaxScaler
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("METHOD 2: MINMAX SCALER")
    print("â”€" * 70)
    
    """
    MinMaxScaler: x_scaled = (x - min) / (max - min)
    
    Result: values in [0, 1]
    
    USE WHEN:
    â€¢ Need bounded values
    â€¢ Data is uniformly distributed
    â€¢ Neural networks (common choice)
    
    CAUTION:
    â€¢ Sensitive to outliers
    """
    
    minmax = MinMaxScaler()
    df_minmax = pd.DataFrame(
        minmax.fit_transform(df),
        columns=df.columns
    )
    
    print("\nAfter MinMaxScaler:")
    print(df_minmax.describe().round(2).to_string())
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # METHOD 3: RobustScaler
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("METHOD 3: ROBUST SCALER")
    print("â”€" * 70)
    
    """
    RobustScaler: x_scaled = (x - median) / IQR
    
    USE WHEN:
    â€¢ Data has outliers
    â€¢ Need robustness
    """
    
    from sklearn.preprocessing import RobustScaler
    
    robust = RobustScaler()
    df_robust = pd.DataFrame(
        robust.fit_transform(df),
        columns=df.columns
    )
    
    print("\nAfter RobustScaler:")
    print(df_robust.describe().round(2).to_string())
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # IMPORTANT: Fit on train, transform both
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("IMPORTANT: PROPER SCALING WORKFLOW")
    print("â”€" * 70)
    
    print("""
    CORRECT WORKFLOW:
    
    1. Split data into train/test FIRST
    2. Fit scaler on training data ONLY
    3. Transform both train and test using the fitted scaler
    
    WHY?
    â€¢ Prevents data leakage (test set info influencing training)
    â€¢ Simulates real-world scenario (you don't know test data at training time)
    
    CODE EXAMPLE:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)  # FIT and TRANSFORM
    X_test_scaled = scaler.transform(X_test)        # ONLY TRANSFORM
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    """)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3.5 TRAIN/TEST SPLIT STRATEGIES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def example_train_test_split():
    """Demonstrate train/test split strategies."""
    
    print("TRAIN/TEST SPLIT STRATEGIES")
    print("=" * 70)
    
    # Sample data
    np.random.seed(42)
    n = 1000
    X = np.random.randn(n, 5)
    y = (X[:, 0] + X[:, 1] + np.random.randn(n) * 0.5 > 0).astype(int)
    
    print(f"\nDataset: {n} samples, {X.shape[1]} features")
    print(f"Class distribution: {np.bincount(y)}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # METHOD 1: Simple Train/Test Split
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("METHOD 1: SIMPLE TRAIN/TEST SPLIT")
    print("â”€" * 70)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    print(f"\nTraining set: {len(X_train)} samples ({len(X_train)/n*100:.0f}%)")
    print(f"Test set: {len(X_test)} samples ({len(X_test)/n*100:.0f}%)")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # METHOD 2: Stratified Split (Preserves class proportions)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("METHOD 2: STRATIFIED SPLIT")
    print("â”€" * 70)
    
    X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print("\nClass proportions:")
    print(f"  Original:  Class 0: {(y==0).mean():.2%}, Class 1: {(y==1).mean():.2%}")
    print(f"  Train:     Class 0: {(y_train_s==0).mean():.2%}, Class 1: {(y_train_s==1).mean():.2%}")
    print(f"  Test:      Class 0: {(y_test_s==0).mean():.2%}, Class 1: {(y_test_s==1).mean():.2%}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # METHOD 3: K-Fold Cross-Validation
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("METHOD 3: K-FOLD CROSS-VALIDATION")
    print("â”€" * 70)
    
    """
    K-Fold CV splits data into K folds.
    Each fold is used as test set once, others as training.
    
    â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
    â”‚ TEST â”‚Train â”‚Train â”‚Train â”‚Train â”‚  Fold 1
    â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
    â”‚Train â”‚ TEST â”‚Train â”‚Train â”‚Train â”‚  Fold 2
    â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
    â”‚Train â”‚Train â”‚ TEST â”‚Train â”‚Train â”‚  Fold 3
    â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
    â”‚Train â”‚Train â”‚Train â”‚ TEST â”‚Train â”‚  Fold 4
    â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
    â”‚Train â”‚Train â”‚Train â”‚Train â”‚ TEST â”‚  Fold 5
    â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
    
    BENEFITS:
    â€¢ Uses all data for training and testing
    â€¢ More reliable performance estimate
    â€¢ Better for small datasets
    """
    
    kfold = KFold(n_splits=5, shuffle=True, random_state=42)
    
    print("\n5-Fold Cross-Validation:")
    for i, (train_idx, test_idx) in enumerate(kfold.split(X)):
        print(f"  Fold {i+1}: Train={len(train_idx)}, Test={len(test_idx)}")
    
    # Stratified K-Fold (for classification)
    print("\nStratified 5-Fold (preserves class distribution in each fold):")
    skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    for i, (train_idx, test_idx) in enumerate(skfold.split(X, y)):
        train_class_dist = np.bincount(y[train_idx]) / len(train_idx)
        test_class_dist = np.bincount(y[test_idx]) / len(test_idx)
        print(f"  Fold {i+1}: Train class 1: {train_class_dist[1]:.2%}, "
              f"Test class 1: {test_class_dist[1]:.2%}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # TIME SERIES SPLIT
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "â”€" * 70)
    print("METHOD 4: TIME SERIES SPLIT")
    print("â”€" * 70)
    
    """
    For time series data, you can't randomly shuffle!
    Training data must come BEFORE test data.
    
    â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
    â”‚Train â”‚ TEST â”‚      â”‚      â”‚      â”‚  Fold 1
    â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
    â”‚Train â”‚Train â”‚ TEST â”‚      â”‚      â”‚  Fold 2
    â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
    â”‚Train â”‚Train â”‚Train â”‚ TEST â”‚      â”‚  Fold 3
    â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
    â”‚Train â”‚Train â”‚Train â”‚Train â”‚ TEST â”‚  Fold 4
    â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
    """
    
    from sklearn.model_selection import TimeSeriesSplit
    
    tscv = TimeSeriesSplit(n_splits=5)
    
    print("\nTime Series Split:")
    for i, (train_idx, test_idx) in enumerate(tscv.split(X)):
        print(f"  Fold {i+1}: Train indices [{train_idx[0]}-{train_idx[-1]}], "
              f"Test indices [{test_idx[0]}-{test_idx[-1]}]")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3.6 CHAPTER 3 SUMMARY
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
CHAPTER 3 SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

KEY TAKEAWAYS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. EDA is essential before modeling
   â€¢ Understand data types, distributions, missing values
   â€¢ Check correlations and class balance
   â€¢ Identify outliers

2. Missing Value Strategies
   â€¢ Deletion: Simple but loses data
   â€¢ Imputation: Mean/median, KNN, model-based
   â€¢ Indicators: Capture missingness as information

3. Outlier Handling
   â€¢ Detection: Z-score, IQR, Isolation Forest
   â€¢ Handling: Remove, cap, transform, or keep

4. Feature Engineering
   â€¢ Mathematical transformations
   â€¢ Date/time extraction
   â€¢ Binning/discretization
   â€¢ Text features
   â€¢ Aggregations

5. Encoding Categorical Variables
   â€¢ Label encoding: Ordinal data
   â€¢ One-hot: Nominal data
   â€¢ Target encoding: High cardinality

6. Feature Scaling
   â€¢ StandardScaler: Most common
   â€¢ MinMaxScaler: Bounded [0,1]
   â€¢ RobustScaler: Handles outliers
   â€¢ ALWAYS fit on train, transform both!

7. Train/Test Splitting
   â€¢ Simple split: 80/20 or 70/30
   â€¢ Stratified: Preserves class distribution
   â€¢ K-Fold CV: More reliable estimates
   â€¢ Time series: Respect temporal order


DATA PREPROCESSING CHECKLIST:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â–¡ Performed EDA
â–¡ Handled missing values
â–¡ Dealt with outliers
â–¡ Created useful features
â–¡ Encoded categorical variables
â–¡ Scaled numerical features
â–¡ Split data properly (with stratification if needed)
â–¡ No data leakage (fit on train only)
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Run all examples
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    print("\n" + "="*70)
    print("CHAPTER 3: DATA FUNDAMENTALS")
    print("="*70)
    
    example_eda_comprehensive()
    print("\n")
    example_handling_missing_values()
    print("\n")
    example_outlier_detection()
    print("\n")
    example_feature_creation()
    print("\n")
    example_categorical_encoding()
    print("\n")
    example_feature_scaling()
    print("\n")
    example_train_test_split()

---

<div align="center">

[â¬…ï¸ Previous: Introduction](00-introduction.md) | [ğŸ“š Table of Contents](../README.md) | [Next: Supervised Learning â¡ï¸](02-supervised-learning.md)

</div>
